{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOLUm7BNMzd0h864KIjQMrY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"eOFUeCKTJWHG"},"outputs":[],"source":["from statistics import mean\n","import pandas as pd\n","from datasets import Dataset\n","from ragas.metrics import faithfulness\n","from ragas import evaluate\n","from ragas.llms import LangchainLLMWrapper\n","from langchain_mistralai import ChatMistralAI\n","\n","# 1. Load your Excel file\n","df = pd.read_excel(\"Questions_with_answers.xlsx\")\n","\n","# 2. Prepare dataset for RAGAS\n","dataset = Dataset.from_dict({\n","    \"question\": df[\"Questions\"].tolist(),\n","    \"answer\": df[\"Answer\"].tolist(),\n","    # RAGAS expects a list of contexts for each question\n","    \"contexts\": df[\"Retrieved Docs\"].apply(lambda x: [str(x)]).tolist()\n","})\n","\n","# 3. Wrap Mistral as a LangChain LLM for RAGAS\n","mistral_llm = ChatMistralAI(\n","    api_key=\"#####\",\n","    model=\"mistral-medium-latest\",\n","    temperature=0\n",")\n","llm_wrapper = LangchainLLMWrapper(mistral_llm)\n","\n","# 4. Evaluate only faithfulness\n","results = evaluate(dataset, metrics=[faithfulness], llm=llm_wrapper)\n","print(results)\n","print(mean(results[\"faithfulness\"]))\n","# 5. Save results back to Excel\n","df[\"Faithfulness\"] = results[\"faithfulness\"]\n","df.to_excel(\"Questions_with_faithfulness.xlsx\", index=False)\n","\n","print(\"Faithfulness scores saved to Questions_with_faithfulness.xlsx\")\n"]}]}