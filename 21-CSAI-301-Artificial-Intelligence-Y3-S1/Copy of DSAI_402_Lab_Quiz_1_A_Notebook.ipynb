{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1Gl8UXf599mjWzWBVftoPhMLuVLtB24UA","timestamp":1762251665918}],"collapsed_sections":["cbz9B-T0t3lu"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#yourNAME, yourID"],"metadata":{"id":"5mJVJ15PuDnJ"}},{"cell_type":"markdown","source":["# Imports"],"metadata":{"id":"BVmskFXtt9ck"}},{"cell_type":"code","source":["import gymnasium as gym\n","from gymnasium import spaces\n","import numpy as np"],"metadata":{"id":"qLc35GaOt8PX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# LuffyDodge (3)"],"metadata":{"id":"B3tcl2yet_iq"}},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"CV2uRUPNoNLy"},"outputs":[],"source":["class LuffyDodgeEnv(gym.Env):\n","    \"\"\"\n","    Custom environment where Luffy must dodge cannonballs.\n","    Luffy can move left, right, or stay still to avoid being hit.\n","    \"\"\"\n","\n","    metadata = {\"render_modes\": [\"human\"]}\n","\n","    def __init__(self, render_mode=None):\n","        \"\"\"\n","        Initialize environment parameters.\n","        \"\"\"\n","        super(LuffyDodgeEnv, self).__init__()\n","\n","        # Actions: 0 = left, 1 = stay, 2 = right\n","        self.action_space = spaces.Discrete(3)\n","\n","        # Observation: [Luffy_x, Cannonball_x, Cannonball_y]\n","        self.observation_space = spaces.Box(\n","            low=np.array([0, 0, 0], dtype=np.float32),\n","            high=np.array([9, 9, 9], dtype=np.float32),\n","            dtype=np.float32\n","        )\n","\n","        self.render_mode = render_mode\n","        self.reset()\n","\n","    def reset(self, seed=None, options=None):\n","        \"\"\"\n","        Reset the environment to its initial state.\n","        \"\"\"\n","        super().reset(seed=seed)\n","\n","        # Luffy starts in the middle bottom\n","        self.luffy_x = 5.0\n","\n","        # Cannonball starts at a random x and top y = 9\n","        self.cannon_x = np.random.randint(0, 10)\n","        self.cannon_y = 9.0\n","\n","        # Return initial observation\n","        obs = np.array([self.luffy_x, self.cannon_x, self.cannon_y], dtype=np.float32)\n","        return obs, {}\n","\n","    def step(self, action):\n","        \"\"\"\n","        Performs one time-step transition in the environment.\n","\n","        Parameters:\n","            action (int): The action taken by Luffy.\n","                          0 = move left, 1 = stay still, 2 = move right\n","\n","        Returns:\n","            observation (np.array): The next state [luffy_x, cannon_x, cannon_y]\n","            reward (float): +1 if survived this step, -10 if hit by cannonball\n","            done (bool): True if Luffy is hit, False otherwise\n","            truncated (bool): Optional flag if max steps are reached\n","            info (dict): Additional info (empty for now)\n","\n","        Notes for Students:\n","        - Update Luffy’s position based on the action.\n","        - Move the cannonball down by one step.\n","        - Check for collision (same x and y == 0).\n","        - If collision occurs, assign -10 reward and mark `done = True`.\n","        - Otherwise, assign +1 reward and continue.\n","        - If the cannonball reaches y = 0 but misses Luffy, reset it to top at a random x position.\n","        \"\"\"\n","       reward=0\n","       done=False\n","       if action=0:\n","            self.luffy_x = max(0, self.luffy_x - 1)\n","       elif action=1:\n","            pass\n","       elif action=2:\n","            self.luffy_x = min(9, self.luffy_x + 1)\n","       self.cannon_y -= 1\n","       if self.cannony==0:\n","            if self.luffy_x==self.cannon_x:\n","                reward=-10\n","                done=True\n","            else:\n","                reward=1\n","                self.cannon_x = np.random.randint(0, 10)\n","      obs=np.array([self.luffy_x, self.cannon_x, self.cannon_y], dtype=np.float32)\n","      return obs, reward, done, False, {}\n","\n","    def render(self):\n","        \"\"\"\n","        Render the environment as simple text output.\n","        \"\"\"\n","        grid = np.full((10, 10), \" \", dtype=str)\n","        grid[int(self.cannon_y), int(self.cannon_x)] = \"O\"\n","        grid[0, int(self.luffy_x)] = \"L\"\n","        print(\"\\n\".join([\"\".join(row) for row in grid[::-1]]))\n","        print(\"-\" * 10)\n","\n","    def close(self):\n","        \"\"\"Close the environment.\"\"\"\n","        pass"]},{"cell_type":"code","source":["env = LuffyDodgeEnv()\n","obs, _ = env.reset()\n","\n","done = False\n","while not done:\n","    action = env.action_space.sample()\n","    obs, reward, done, truncated, info = env.step(action)\n","    env.render()\n","    print(f\"Action: {action}, Reward: {reward}\")"],"metadata":{"id":"Px1liZDDt0Tb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762249432166,"user_tz":-120,"elapsed":805,"user":{"displayName":"Abdalrahman Omer 202202254","userId":"05776575457129899449"}},"outputId":"8f72e9a6-fcc7-49b8-833d-f3129ea34706"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["          \n","         O\n","          \n","          \n","          \n","          \n","          \n","          \n","          \n","     L    \n","----------\n","Action: 1, Reward: 1.0\n","          \n","          \n","         O\n","          \n","          \n","          \n","          \n","          \n","          \n","     L    \n","----------\n","Action: 1, Reward: 1.0\n","          \n","          \n","          \n","         O\n","          \n","          \n","          \n","          \n","          \n","    L     \n","----------\n","Action: 0, Reward: 1.0\n","          \n","          \n","          \n","          \n","         O\n","          \n","          \n","          \n","          \n","    L     \n","----------\n","Action: 1, Reward: 1.0\n","          \n","          \n","          \n","          \n","          \n","         O\n","          \n","          \n","          \n","     L    \n","----------\n","Action: 2, Reward: 1.0\n","          \n","          \n","          \n","          \n","          \n","          \n","         O\n","          \n","          \n","      L   \n","----------\n","Action: 2, Reward: 1.0\n","          \n","          \n","          \n","          \n","          \n","          \n","          \n","         O\n","          \n","     L    \n","----------\n","Action: 0, Reward: 1.0\n","          \n","          \n","          \n","          \n","          \n","          \n","          \n","          \n","         O\n","     L    \n","----------\n","Action: 1, Reward: 1.0\n"," O        \n","          \n","          \n","          \n","          \n","          \n","          \n","          \n","          \n","      L   \n","----------\n","Action: 2, Reward: 1.0\n","          \n"," O        \n","          \n","          \n","          \n","          \n","          \n","          \n","          \n","      L   \n","----------\n","Action: 1, Reward: 1.0\n","          \n","          \n"," O        \n","          \n","          \n","          \n","          \n","          \n","          \n","       L  \n","----------\n","Action: 2, Reward: 1.0\n","          \n","          \n","          \n"," O        \n","          \n","          \n","          \n","          \n","          \n","       L  \n","----------\n","Action: 1, Reward: 1.0\n","          \n","          \n","          \n","          \n"," O        \n","          \n","          \n","          \n","          \n","        L \n","----------\n","Action: 2, Reward: 1.0\n","          \n","          \n","          \n","          \n","          \n"," O        \n","          \n","          \n","          \n","         L\n","----------\n","Action: 2, Reward: 1.0\n","          \n","          \n","          \n","          \n","          \n","          \n"," O        \n","          \n","          \n","        L \n","----------\n","Action: 0, Reward: 1.0\n","          \n","          \n","          \n","          \n","          \n","          \n","          \n"," O        \n","          \n","       L  \n","----------\n","Action: 0, Reward: 1.0\n","          \n","          \n","          \n","          \n","          \n","          \n","          \n","          \n"," O        \n","       L  \n","----------\n","Action: 1, Reward: 1.0\n","     O    \n","          \n","          \n","          \n","          \n","          \n","          \n","          \n","          \n","       L  \n","----------\n","Action: 1, Reward: 1.0\n","          \n","     O    \n","          \n","          \n","          \n","          \n","          \n","          \n","          \n","      L   \n","----------\n","Action: 0, Reward: 1.0\n","          \n","          \n","     O    \n","          \n","          \n","          \n","          \n","          \n","          \n","     L    \n","----------\n","Action: 0, Reward: 1.0\n","          \n","          \n","          \n","     O    \n","          \n","          \n","          \n","          \n","          \n","    L     \n","----------\n","Action: 0, Reward: 1.0\n","          \n","          \n","          \n","          \n","     O    \n","          \n","          \n","          \n","          \n","    L     \n","----------\n","Action: 1, Reward: 1.0\n","          \n","          \n","          \n","          \n","          \n","     O    \n","          \n","          \n","          \n","     L    \n","----------\n","Action: 2, Reward: 1.0\n","          \n","          \n","          \n","          \n","          \n","          \n","     O    \n","          \n","          \n","      L   \n","----------\n","Action: 2, Reward: 1.0\n","          \n","          \n","          \n","          \n","          \n","          \n","          \n","     O    \n","          \n","       L  \n","----------\n","Action: 2, Reward: 1.0\n","          \n","          \n","          \n","          \n","          \n","          \n","          \n","          \n","     O    \n","       L  \n","----------\n","Action: 1, Reward: 1.0\n","         O\n","          \n","          \n","          \n","          \n","          \n","          \n","          \n","          \n","      L   \n","----------\n","Action: 0, Reward: 1.0\n","          \n","         O\n","          \n","          \n","          \n","          \n","          \n","          \n","          \n","     L    \n","----------\n","Action: 0, Reward: 1.0\n","          \n","          \n","         O\n","          \n","          \n","          \n","          \n","          \n","          \n","      L   \n","----------\n","Action: 2, Reward: 1.0\n","          \n","          \n","          \n","         O\n","          \n","          \n","          \n","          \n","          \n","     L    \n","----------\n","Action: 0, Reward: 1.0\n","          \n","          \n","          \n","          \n","         O\n","          \n","          \n","          \n","          \n","     L    \n","----------\n","Action: 1, Reward: 1.0\n","          \n","          \n","          \n","          \n","          \n","         O\n","          \n","          \n","          \n","      L   \n","----------\n","Action: 2, Reward: 1.0\n","          \n","          \n","          \n","          \n","          \n","          \n","         O\n","          \n","          \n","      L   \n","----------\n","Action: 1, Reward: 1.0\n","          \n","          \n","          \n","          \n","          \n","          \n","          \n","         O\n","          \n","     L    \n","----------\n","Action: 0, Reward: 1.0\n","          \n","          \n","          \n","          \n","          \n","          \n","          \n","          \n","         O\n","      L   \n","----------\n","Action: 2, Reward: 1.0\n","     O    \n","          \n","          \n","          \n","          \n","          \n","          \n","          \n","          \n","     L    \n","----------\n","Action: 0, Reward: 1.0\n","          \n","     O    \n","          \n","          \n","          \n","          \n","          \n","          \n","          \n","     L    \n","----------\n","Action: 1, Reward: 1.0\n","          \n","          \n","     O    \n","          \n","          \n","          \n","          \n","          \n","          \n","     L    \n","----------\n","Action: 1, Reward: 1.0\n","          \n","          \n","          \n","     O    \n","          \n","          \n","          \n","          \n","          \n","    L     \n","----------\n","Action: 0, Reward: 1.0\n","          \n","          \n","          \n","          \n","     O    \n","          \n","          \n","          \n","          \n","   L      \n","----------\n","Action: 0, Reward: 1.0\n","          \n","          \n","          \n","          \n","          \n","     O    \n","          \n","          \n","          \n","   L      \n","----------\n","Action: 1, Reward: 1.0\n","          \n","          \n","          \n","          \n","          \n","          \n","     O    \n","          \n","          \n","    L     \n","----------\n","Action: 2, Reward: 1.0\n","          \n","          \n","          \n","          \n","          \n","          \n","          \n","     O    \n","          \n","   L      \n","----------\n","Action: 0, Reward: 1.0\n","          \n","          \n","          \n","          \n","          \n","          \n","          \n","          \n","     O    \n","   L      \n","----------\n","Action: 1, Reward: 1.0\n"," O        \n","          \n","          \n","          \n","          \n","          \n","          \n","          \n","          \n","   L      \n","----------\n","Action: 1, Reward: 1.0\n","          \n"," O        \n","          \n","          \n","          \n","          \n","          \n","          \n","          \n","    L     \n","----------\n","Action: 2, Reward: 1.0\n","          \n","          \n"," O        \n","          \n","          \n","          \n","          \n","          \n","          \n","   L      \n","----------\n","Action: 0, Reward: 1.0\n","          \n","          \n","          \n"," O        \n","          \n","          \n","          \n","          \n","          \n","  L       \n","----------\n","Action: 0, Reward: 1.0\n","          \n","          \n","          \n","          \n"," O        \n","          \n","          \n","          \n","          \n"," L        \n","----------\n","Action: 0, Reward: 1.0\n","          \n","          \n","          \n","          \n","          \n"," O        \n","          \n","          \n","          \n","L         \n","----------\n","Action: 0, Reward: 1.0\n","          \n","          \n","          \n","          \n","          \n","          \n"," O        \n","          \n","          \n","L         \n","----------\n","Action: 0, Reward: 1.0\n","          \n","          \n","          \n","          \n","          \n","          \n","          \n"," O        \n","          \n","L         \n","----------\n","Action: 1, Reward: 1.0\n","          \n","          \n","          \n","          \n","          \n","          \n","          \n","          \n"," O        \n","L         \n","----------\n","Action: 0, Reward: 1.0\n","          \n","          \n","          \n","          \n","          \n","          \n","          \n","          \n","          \n"," L        \n","----------\n","Action: 2, Reward: -10.0\n"]}]},{"cell_type":"markdown","source":["# Policy Evaluation (2)"],"metadata":{"id":"cbz9B-T0t3lu"}},{"cell_type":"code","source":["# Grid size\n","GRID_SIZE = 5\n","\n","# Actions: 0 = left, 1 = stay, 2 = right\n","ACTIONS = [0, 1, 2]\n","NUM_ACTIONS = len(ACTIONS)\n","\n","# All possible states: (Luffy_x, Cannon_x, Cannon_y)\n","states = [(lx, cx, cy) for lx in range(GRID_SIZE)\n","                         for cx in range(GRID_SIZE)\n","                         for cy in range(GRID_SIZE)]\n","NUM_STATES = len(states)\n","\n","# Initialize policy randomly\n","policy = np.random.choice(ACTIONS, size=NUM_STATES)\n","\n","# Initialize value function\n","V = np.zeros(NUM_STATES)\n","\n","# Discount factor\n","gamma = 0.9\n","\n","\n","def state_to_index(state):\n","    \"\"\"Convert a (luffy_x, cannon_x, cannon_y) state tuple into its index in the state list.\"\"\"\n","    lx, cx, cy = state\n","    return lx * GRID_SIZE * GRID_SIZE + cx * GRID_SIZE + cy\n","\n","\n","def transition(state, action):\n","    \"\"\"\n","    Deterministic transition function for the environment.\n","\n","    Parameters\n","    ----------\n","    state : tuple (luffy_x, cannon_x, cannon_y)\n","    action : int (0=left, 1=stay, 2=right)\n","\n","    Returns\n","    -------\n","    next_state : tuple or None\n","        Next state after taking the action.\n","    reward : float\n","        Reward received after the transition.\n","    done : bool\n","        Whether the episode terminates (Luffy hit by cannonball).\n","    \"\"\"\n","    lx, cx, cy = state\n","\n","    # Move Luffy\n","    if action == 0:\n","        lx = max(0, lx - 1)\n","    elif action == 2:\n","        lx = min(GRID_SIZE - 1, lx + 1)\n","\n","    # Move cannonball down\n","    cy -= 1\n","\n","    # Check terminal condition\n","    if cy < 0:\n","        if lx == cx:\n","            # Hit\n","            return None, -10.0, True\n","        else:\n","            # Miss → reset cannonball to top\n","            return (lx, cx, GRID_SIZE - 1), +1.0, False\n","\n","    # Otherwise just one step closer to bottom\n","    return (lx, cx, cy), +1.0, False\n","\n","\n","def policy_evaluation(policy, V, theta=1e-4):\n","    \"\"\"\n","    Iteratively evaluate the value function for the given policy.\n","    \"\"\"\n","    while True:\n","        delta = 0\n","        for s, state in enumerate(states):\n","            v = V[s]  # old value\n","            action = policy[s]\n","\n","            next_state, reward, done = transition(state, action)\n","\n","            if done or next_state is None:\n","                V[s] = reward\n","            else:\n","                V[s] = reward + gamma * V[state_to_index(next_state)]\n","\n","            delta = max(delta, abs(v - V[s]))\n","\n","        if delta < theta:\n","            break\n","    return V\n","\n","\n","\n","def policy_improvement(V, policy):\n","    \"\"\"Greedy policy improvement based on the current value function.\"\"\"\n","    policy_stable = True\n","    for s, state in enumerate(states):\n","        old_action = policy[s]\n","        action_values = []\n","\n","        # Try all actions and pick the best one\n","        for a in ACTIONS:\n","            next_state, reward, done = transition(state, a)\n","            if done:\n","                action_values.append(reward)\n","            else:\n","                action_values.append(reward + gamma * V[state_to_index(next_state)])\n","\n","        best_action = np.argmax(action_values)\n","        policy[s] = best_action\n","\n","        # Check if policy changed\n","        if old_action != best_action:\n","            policy_stable = False\n","    return policy, policy_stable\n","\n","\n","def policy_iteration():\n","    \"\"\"Run the full policy iteration loop.\"\"\"\n","    global V, policy\n","    iteration = 0\n","    while True:\n","        iteration += 1\n","        V = policy_evaluation(policy, V)\n","        policy, stable = policy_improvement(V, policy)\n","        print(f\"Iteration {iteration} completed.\")\n","        if stable:\n","            print(\"✅ Policy converged!\")\n","            break\n","    return policy, V\n","\n","\n","if __name__ == \"__main__\":\n","    optimal_policy, optimal_V = policy_iteration()\n","\n","    print(\"\\nOptimal policy (sample of 10 states):\")\n","    for i in range(10):\n","        print(f\"State {states[i]} → Action {optimal_policy[i]}\")\n"],"metadata":{"id":"8qV9XOHIpzUn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762250462605,"user_tz":-120,"elapsed":161,"user":{"displayName":"Abdalrahman Omer 202202254","userId":"05776575457129899449"}},"outputId":"0933e5c0-52a3-44d9-fdf5-b149feb5144d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Iteration 1 completed.\n","Iteration 2 completed.\n","Iteration 3 completed.\n","Iteration 4 completed.\n","Iteration 5 completed.\n","Iteration 6 completed.\n","Iteration 7 completed.\n","Iteration 8 completed.\n","Iteration 9 completed.\n","Iteration 10 completed.\n","Iteration 11 completed.\n","Iteration 12 completed.\n","Iteration 13 completed.\n","Iteration 14 completed.\n","Iteration 15 completed.\n","Iteration 16 completed.\n","Iteration 17 completed.\n","Iteration 18 completed.\n","Iteration 19 completed.\n","Iteration 20 completed.\n","Iteration 21 completed.\n","Iteration 22 completed.\n","Iteration 23 completed.\n","Iteration 24 completed.\n","Iteration 25 completed.\n","Iteration 26 completed.\n","Iteration 27 completed.\n","Iteration 28 completed.\n","Iteration 29 completed.\n","Iteration 30 completed.\n","Iteration 31 completed.\n","Iteration 32 completed.\n","Iteration 33 completed.\n","Iteration 34 completed.\n","Iteration 35 completed.\n","Iteration 36 completed.\n","Iteration 37 completed.\n","Iteration 38 completed.\n","Iteration 39 completed.\n","Iteration 40 completed.\n","Iteration 41 completed.\n","Iteration 42 completed.\n","Iteration 43 completed.\n","Iteration 44 completed.\n","Iteration 45 completed.\n","Iteration 46 completed.\n","Iteration 47 completed.\n","Iteration 48 completed.\n","Iteration 49 completed.\n","Iteration 50 completed.\n","Iteration 51 completed.\n","Iteration 52 completed.\n","Iteration 53 completed.\n","Iteration 54 completed.\n","Iteration 55 completed.\n","✅ Policy converged!\n","\n","Optimal policy (sample of 10 states):\n","State (0, 0, 0) → Action 2\n","State (0, 0, 1) → Action 0\n","State (0, 0, 2) → Action 0\n","State (0, 0, 3) → Action 0\n","State (0, 0, 4) → Action 0\n","State (0, 1, 0) → Action 0\n","State (0, 1, 1) → Action 0\n","State (0, 1, 2) → Action 0\n","State (0, 1, 3) → Action 0\n","State (0, 1, 4) → Action 0\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"smMfLwu3UBNz"},"execution_count":null,"outputs":[]}]}