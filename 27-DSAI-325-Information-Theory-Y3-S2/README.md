# DSAI 325 - Information Theory (Y3-S2)

<div align="center">

**Spring 2024** | **Theoretical Foundation**

[![Materials](https://img.shields.io/badge/Materials-Complete-success)](.)
[![Skills](https://img.shields.io/badge/Skills-Entropy%20|%20Coding%20|%20Channels-orange)](.)

</div>

---

## ğŸ“‹ Course Overview

Mathematical foundations of information theory covering entropy, mutual information, coding theory, and channel capacity. Provides theoretical underpinnings for understanding compression, communication limits, and information-theoretic approaches in ML.

---

## ğŸ¯ Learning Outcomes Achieved

### Information Measures
- âœ… Entropy and conditional entropy
- âœ… Mutual information
- âœ… KL divergence
- âœ… Information-theoretic inequalities

### Coding Theory
- âœ… Source coding theorems
- âœ… Huffman coding
- âœ… Channel coding
- âœ… Error-correcting codes

### Applications
- âœ… Data compression
- âœ… Channel capacity analysis
- âœ… Information bottleneck
- âœ… ML applications

---

## ğŸ“š Topics Covered

### Module 1: Entropy & Information (Weeks 1-4)
- Entropy definitions
- Joint and conditional entropy
- Mutual information
- Chain rules

### Module 2: Source Coding (Weeks 5-8)
- Source coding theorem
- Huffman codes
- Arithmetic coding
- Lempel-Ziv compression

### Module 3: Channel Coding (Weeks 9-12)
- Channel models
- Channel capacity theorem
- Error-correcting codes
- Noisy channel coding

### Module 4: ML Applications (Weeks 13-14)
- Information bottleneck
- Rate-distortion theory
- Variational inference connection
- Representation learning

---

## ğŸ¯ Course Impact on Graduate Studies Preparation

Information theory provides foundation for:
- Understanding VAEs and information bottleneck
- Compression techniques in ML
- Theoretical analysis of learning
- Mutual information estimation

---

<div align="center">

**Key Takeaway**: Information theory provides fundamental limits and insights for learning systems.

[â¬…ï¸ Back to All Courses](../README.md#-complete-course-catalog)

</div>
