{"cells":[{"cell_type":"markdown","metadata":{"id":"rnLSMJ-G78tL"},"source":["<h1 align=\"center\"> Zewail City of Science and Technology</h1>\n","<h2 align=\"center\">Lab 2: KNN </h2>\n"]},{"cell_type":"markdown","metadata":{"id":"cCp1rxch78tM"},"source":["# Lab Objectives"]},{"cell_type":"markdown","metadata":{"id":"Zwwk3Fuk78tM"},"source":["- Explain the difference between the parametric and the non-parametric models.\n","\n","- Classification\n","- KNN Algorithm\n","- Notes on KNN:\n","> 1. Distance (manhattan & euclidean)\n","> 2. Choosing K\n","> 3. Implement the KNN algorithm from scratch\n","> 4. The curse of dimensionality\n","> 5. Summarize KNN pros and cons\n","\n","- Explore Iris Dataset, one of the most widely used for educational purposes.\n","\n","- Should we normalize our data?\n","\n","- Compare the result with the sklearn library.\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"594b36db"},"source":["# Parametric VS non-parametric models\n","- In parametric models:\n","> The function that should fit the data is known. <br>\n","> For example, in linear regression, we know that we should fit the data with a line with the following equation: $ y = WX + b $\n","- In non-parametric models:\n","> The data speakes for itself where a function $F(X)$ will fit your data. $F(X)$ can then be approximated to a function with any number of parameters."]},{"cell_type":"markdown","metadata":{"id":"717224ab"},"source":["# What is Classification?\n","<img src=\"https://miro.medium.com/max/1400/0*m85bAFJiPG7Z0L3w.png\" align = 'center' style=\"width: 600px;\">"]},{"cell_type":"markdown","metadata":{"id":"44347cbf"},"source":["## KNN Algorithm\n","> 1. Calculate the distances between all the training data and the test point.\n","> 2. Find the nearest K neighbors by sorting these pairwise distances.\n","> 3. Classify the point based on a majority vote.\n","\n","<img src=\"https://miro.medium.com/max/591/0*DgJ3Xx6QzWleIkXC.png\" align = 'right' style =\"500px\">"]},{"cell_type":"markdown","metadata":{"id":"0de46fbc"},"source":["You can check this link for a demo: http://vision.stanford.edu/teaching/cs231n-demos/knn/"]},{"cell_type":"markdown","metadata":{"id":"8be78164"},"source":["### Notes on KNN   \n",">\n","1. **How calculate the distance?**\n","> **Minkowski Distance:**\n","> - If p is set to 1, we get the Manhattan distance. It is preferred when the features are not of a similar type (such as age, gender, height...etc) or when the dimension of the data is huge.\n","> - If p is set to 2, we get the Euclidean distance. It is used when the features are of a similar type (width, height, depth,..etc). <br>\n","<img src=\"https://www.kdnuggets.com/wp-content/uploads/popular-knn-metrics-1.png\" align = 'center'>\n","2. **Choosing K:**\n","> If K is too small, the model will be sensitive to noise (high variance, low bias) <br>\n","> If K is large, the model may include instances from other classes (high bias, low variance).\n","<img src=\"https://www.fromthegenesis.com/wp-content/uploads/2018/09/K_NN_Ad.jpg\" align = 'center' style =\"100px\">\n","3. **The curse of dimensionality:** <br>\n","In KNN, the number of training samples should increase with increasing the dimension. The challenge in KNN is that it requires a neighbour point to be close in every single dimension. And adding a new dimension makes it harder and harder for two points to be close to each other in every axis. To solve this problem, one can reduce the # of irrelevant features.\n"]},{"cell_type":"markdown","metadata":{"id":"_v5S6MHhulUu"},"source":["# Importing the needed libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"29bb9faf"},"outputs":[],"source":["#import needed libraries (pandas , numpy , seaborn, matplotlib , sklearn)\n","\n","\n","\n","from collections import Counter\n","\n","from sklearn.metrics import accuracy_score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nDe4T6VrtxS6"},"outputs":[],"source":["!pip install -U scikit-learn"]},{"cell_type":"markdown","metadata":{"id":"gfHf4Oj878tU"},"source":["# Exploring the Iris dataset"]},{"cell_type":"markdown","metadata":{"id":"dMkxnCVG78tU"},"source":["We are going to use a very famous dataset called Iris"]},{"cell_type":"markdown","metadata":{"id":"kFdQdR1378tU"},"source":["##### Attributes:\n","1. sepal length in cm\n","2. sepal width in cm\n","3. petal length in cm\n","4. petal width in cm\n","\n","We will just use two features for easier visualization; sepal length and width.\n","\n","##### class:\n","* Iris Setosa\n","* Iris Versicolour\n","* Iris Virginica\n","\n","<img src=\"https://miro.medium.com/max/1000/1*Hh53mOF4Xy4eORjLilKOwA.png\" align = 'right'>\n"]},{"cell_type":"markdown","metadata":{"id":"bVpAGd6S78tV"},"source":["## Load the Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0ULKI6gk78tV"},"outputs":[],"source":["# import iris dataset\n","iris =\n","print(type(iris))\n","\n","# use the numpy concatenate function\n","iris_df = pd.DataFrame( )\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cR4_KDcSwjlx"},"outputs":[],"source":["# Show the dataset infromation\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sQSFPR1XwkMl"},"outputs":[],"source":["#Show head of dataset\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jtzWdOCO78tZ"},"outputs":[],"source":["#Describe the dataset\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dnpXJUwttxS_"},"outputs":[],"source":["#check the samples for each class / is it balanced dataset\n","iris_df."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H3O8c4N0txTA"},"outputs":[],"source":["#check for missing data\n"]},{"cell_type":"code","source":["#check duplicates\n"],"metadata":{"id":"wgj4a8PYv_5z"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1Rx2bi57txTA","outputId":"28f6e4e8-8c5e-4d46-d7b3-8bd761124a17"},"outputs":[{"output_type":"stream","name":"stdout","text":["duplicate values -> 0\n"]}],"source":["#drop duplicates\n","\n","#test after remove the duplicates\n"]},{"cell_type":"markdown","metadata":{"id":"7-I6Ki1478tc"},"source":["## Split into X and Y"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vvGdaZdo78td"},"outputs":[],"source":["##select all rows and all columns except the last one.\n","X = iris_df.iloc\n","\n","##select all rows, but only the last column.\n","y ="]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JT8xwzKg78th","scrolled":false},"outputs":[],"source":["#check the X head\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L04bGIWz78tk"},"outputs":[],"source":["#check the y head\n"]},{"cell_type":"markdown","metadata":{"id":"LDoQX-B278tn"},"source":["## Split into training and testing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wiTnDskI78to"},"outputs":[],"source":["#split the data into train and test sets (80,20):\n","#Shuffle=True, meaning the data will be shuffled before splitting.\n","\n","................. = train_test_split(.......)\n","\n","X_train = np.asarray(X_train)\n","y_train = np.asarray(y_train)\n","\n","X_test = np.asarray(X_test)\n","y_test = np.asarray(y_test)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0qOdPgXC78tr","outputId":"4ed40b84-51d8-449c-d1fd-8259ce4c6702"},"outputs":[{"output_type":"stream","name":"stdout","text":["training set size: 119 samples \n","test set size: 30 samples\n"]}],"source":["#check the traing set size and test set size:\n"]},{"cell_type":"markdown","metadata":{"id":"B3OY3F8v78tv"},"source":["## Normalize the dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Un4wPlFp78tv"},"outputs":[],"source":["#the scaler is fitted to the training set / the Normalizer calculates the normalization parameters based on the training set.\n","\n","#the scaler is applied to the training set / this step scales each feature in the training set independently.\n","\n","##the scaler is applied to the test set\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Khkxm-NA78t0"},"outputs":[],"source":["print(\"X train before Normalization\")\n","print(X_train[0:5])\n","print(\"\\nX train after Normalization\")\n","print(normalized_X_train[0:5])"]},{"cell_type":"markdown","metadata":{"id":"tYj_EAkLtxTD"},"source":["## Should we normalize our data?\n","Please check: https://stats.stackexchange.com/questions/287425/why-do-you-need-to-scale-data-in-knn/287439\n","<img src=\"https://i.stack.imgur.com/OCUmI.png\" align = 'left' style=\"100px\">\n","<img src=\"https://i.stack.imgur.com/J5r01.png\" align = 'right' style=\"100px\">"]},{"cell_type":"markdown","metadata":{"id":"UP9r-GNn78t2"},"source":["## Visualize the dataset before and after normalization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ou_JYw4378t3"},"outputs":[],"source":["## Before\n","# view the relationships between variables; color code by species type\n","di = {0.0: \"Setosa\", 1.0: \"Versicolor\", 2.0 : \"Virginica\"}\n","\n","before = sns.pairplot(iris_df.replace({\"target\": di}), hue='target')\n","before.fig.suptitle(\"Pair Plot of the dataset Before normalization\", y=1.08)\n","\n","\n"]},{"cell_type":"code","source":["## After\n","iris_df_2 = pd.DataFrame(data= np.c_[normalized_X_train, y_train],\n","                          columns = iris['feature_names'] + ['target'] )\n","di = {0.0: \"Setosa\", 1.0: \"Versicolor\", 2.0 : \"Virginica\"}\n","after = sns.pairplot(iris_df_2.replace({\"target\": di}), hue='target')\n","after.fig.suptitle(\"Pair Plot of the dataset After normalization\", y=1.08)"],"metadata":{"id":"X4C_7RixxenV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#check corroleation\n"],"metadata":{"id":"80u6moMcxzZ7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#visualize heatmap for different features before normalise\n","fig, ax = plt.subplots(figsize=(15,5))\n","\n","sns.heatmap();\n","plt.title()\n"],"metadata":{"id":"LggFdAyPx7zA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ozv-r4VitxTN"},"source":["Observations:\n","\n","    Petal length & width are highly correlated with eachother.\n","    Petal length & width are highly correlated with the target.\n","    Petal length & width are also correlated with the sepal length, but not the width.\n","    We will work on two features only: petal length & width for better visualization.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"hhLTbpDi78t6"},"source":["# Implemenet KNN from scratch"]},{"cell_type":"markdown","metadata":{"id":"i5a4smaP78t7"},"source":["## KNN Algorithm steps"]},{"cell_type":"markdown","metadata":{"id":"O0FhsgWm78t7"},"source":["1. Calculate the distances between all the training data and the test point.\n","\n","\n","2. Find the nearest K neighbors by sorting these pairwise distances.\n","\n","\n","3. Classify the point based on a majority vote."]},{"cell_type":"markdown","metadata":{"id":"RYAS6Zlz78t8"},"source":["### Step 1 ( Calculate the distance using Eculidean distance)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EraTpShd78t9"},"outputs":[],"source":["def distance_ecu(.... , ....):\n","    \"\"\"\n","    Input:\n","        - x_train : corresponding to the training data\n","        - x_test_point : corresponding to the test point\n","\n","    Output:\n","        - distances : The distances between the the test point and each point in the training data.\n","\n","    \"\"\"\n","    distances = []\n","    ## Loop over the rows of X train\n","    for ... in range(.....):\n","\n","        # Get them point by point\n","        current_train_point =  .....\n","\n","        # Initialize the distance by zero\n","        current_distance = .....\n","\n","        # Loop over the columns of the row\n","        for ... in range(.....):\n","\n","            current_distance +=\n","\n","        current_distance = ....\n","\n","        ## Append the distances !\n","        distances.append(.....)\n","\n","    # Store distances in a dataframe\n","    distances = pd.DataFrame(data=distances, columns=['index'])\n","    return ....."]},{"cell_type":"markdown","metadata":{"id":"ZwurNAE778t_"},"source":["### Step 2 ( Find the nearest neighbors )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bdu7IGqV78uA"},"outputs":[],"source":["def nearest_neighbors(..... , ..):\n","    \"\"\"\n","    Input:\n","        - distance_point : The distances between the the test point and each point in the training data.\n","        - K              : The number of neighbors\n","\n","    Output:\n","        - df_nearest : The nearest K neighbors between the test point and the training data\n","\n","    \"\"\"\n","    # Sort distances using the sort_values function\n","    df_nearest = ....\n","\n","    ## Take only the first K neighbors\n","    df_nearest = ....\n","    return ...."]},{"cell_type":"markdown","metadata":{"id":"mSLeTfTs78uC"},"source":["### Step 3 ( Classify the point based on a majority vote )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UwAm0g8d78uD"},"outputs":[],"source":["def voting(.... , ....):\n","    \"\"\"\n","    Input:\n","        - df_nearest: Dataframe contains the nearest K neighbors between the Full training dataset and the test point\n","        - y_train : The labels of the training dataset\n","\n","    Output:\n","        - y_pred : The prediction based on Majority Voting\n","\n","    \"\"\"\n","    ## Use the Counter Object to get the labels with K nearest neighbors\n","    counter_vote  = .......\n","    ## Majority Voting !\n","    y_pred = ....\n","\n","\n","    return ...."]},{"cell_type":"markdown","metadata":{"id":"zkAtTrZe78uH"},"source":["## KNN Full Algorithm : Putting Everything Together"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QaJH83Vx78uH"},"outputs":[],"source":["def KNN_from_scratch(.....):\n","\n","    \"\"\"\n","    Input:\n","    - x_train: The full training dataset\n","    - y_train : The labels of the training dataset\n","    - x_test : The full test dataset\n","    - K : The number of neighbors\n","\n","    Output:\n","    - y_pred : The prediction for the whole test set based on Majority Voting\n","    \"\"\"\n","    y_pred = ...\n","\n","    ## Loop over all the test set and perform the three steps\n","    for .. in ..:\n","        ## Step 1\n","        distance_point   =\n","        ## step 2\n","        df_nearest_point =\n","        ## Step 3\n","        y_pred_point     =\n","        y_pred.\n","\n","    return .."]},{"cell_type":"markdown","metadata":{"id":"Qo98lVER78uK"},"source":["## Test the KNN Algorithm on the test dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IcPxrITg78uL"},"outputs":[],"source":["#test at k = 3\n","K = 3\n","y_pred_scratch =\n","\n","print()"]},{"cell_type":"markdown","metadata":{"id":"fqhGI1dK78uN"},"source":["## Compare our implementation with the Sklearn library"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eD-Qqu_B78uO"},"outputs":[],"source":["knn= ....\n","knn.fit(...)\n","y_pred_sklearn = ...\n","print(..)"]},{"cell_type":"markdown","metadata":{"id":"yLvsFbPq78uU"},"source":["### Check if the output is exactly the same"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u-_3eu4778uV"},"outputs":[],"source":["print()"]},{"cell_type":"markdown","metadata":{"id":"0xCRmeJE78uY"},"source":["### Calculate the accuracy of both methods"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ViJV0or178uY"},"outputs":[],"source":["print(f'The accuracy of our implementation is {accuracy_score(....)}')\n","print(f'The accuracy of sklearn implementation is {accuracy_score(...)}')"]},{"cell_type":"markdown","metadata":{"id":"T52VJM6q78uy"},"source":["# The advantages of KNN"]},{"cell_type":"markdown","metadata":{"id":"jAMadpOT78uz"},"source":["- k nearest neighbors is a **lazy learner** because it doesn't learn anything (No training period)\n","\n","- New data can be added smoothly which will not impact the algorithm\n","\n","- It is easy to implement\n","\n","- Otherwise, the features with the largest magnitudes will dominate the total (euclidean) distance. Unless we use Manhatten distance.\n"]},{"cell_type":"markdown","source":["# The disadantages of KNN"],"metadata":{"id":"7CWPkM1Q6-Pa"}},{"cell_type":"markdown","metadata":{"id":"MwmfrElt7pxb"},"source":["- Does not work with large dataset\n","\n","- Does not work well with higher dimensions\n","\n","- Needs feature scaling (normalization and standardization)\n","\n","- Sensitive to null and missing vaues\n"]},{"cell_type":"markdown","metadata":{"id":"gvABCo0Y78u-"},"source":["# KNN Use Cases"]},{"cell_type":"markdown","metadata":{"id":"eMT9JZFx78u_"},"source":["## 1. Text mining:\n","### [Paper 1 : KNN based Machine Learning Approach for Text and Document Mining](https://pdf.sciencedirectassets.com/278653/1-s2.0-S1877705814X00020/1-s2.0-S1877705814003750/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjELz%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJGMEQCIAYysWGkW9lcSdIdwftse0BnGkXXkh9%2FaR6lRGPkSNX7AiBUeVVRvH9T7p3BLu7vTDvlqW3quRCRsA6ikQDr48KANSq9Awj1%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F8BEAMaDDA1OTAwMzU0Njg2NSIMMI%2F3QbJRCmCQ1%2BHqKpEDWGyuoZOZrkuD3vjNmi0V5JoT19%2Fcemy3TO32%2BZ%2BA%2Fb%2FEtYa0Acj25e1hjmG2zehJ8aimibtt3nUDNo7pcZvxZJp9yAUgRP4VZbT4Pp3bQVqQMFm6ULbskJy%2BdFyc%2FBbeNRXq6OMhjzYjlYgt8%2BNPGG66qLyXjJxCPdjrH42FOrAYi0pQMe5qqw6Gt%2FWnqYZJFKjLrZxKXxJg%2FIdtXjYM8SJTByP9%2FSnGQ%2F9jfVot0bw1SqPPHO1IYXb1CmxdaF8PTVb4O6pfZr1ehT2SK4LrnBx3EO2RV2OpUMP1oguMPlfK1C9UuuvxsU%2BNj5IKst9%2FoQB6mLLFWoRhpb2o9uAzYwrCItxLvTtyQ0KY1EAkxTIpY3fwj0BTqFupspPM9yHw5lnOAgWbb4a4ABxqCEaIVQVMhyQ%2BFcjYaN%2BJAMntgXI8lOST5UXID1qeU2C17T1h%2FgSJOxaAejkkDBgy6jUgT%2FO8khpPHqDGDeHt9G%2Fwk5OGDzcKyXHJC1dm5wVTcd0oGldQrrkL%2Fn%2FOF%2Fw2EzLo018w2%2Fin%2FAU67AEDvrhaYpv4A8H3ZqBJ9D8P06AV9UqZbSgjG22m7GVRk%2B8z%2FlxzVaYEbJg1JxiBRAEVOGo0aKodsYGIXfnVdirK%2FeFkRqedTkLMraH%2FVOTJYzk8gLG2AVGBrMh%2Be4Kmu%2Bubea1%2BWC9C%2FEBBx922UjAaY%2BEZvaTjk1yazfbkGAouDIowCnC5STgzVLuW9zD3mPYcbDFKMg4OLIfgirLkIPWgK6S3QlGV9NEnWxyf3vMo8fhQPk7EgUACNCqmSd2fEt%2F1tOvge6SmJHzN5cNIOM48YZZcoJZXOQqWbjwhPU0d4jCwUoZMOEsTMKWStQ%3D%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20201016T213143Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTYUISC45GZ%2F20201016%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=968f068ec1db669ac8d502e339a188fe7f68aeda68c4947583b12a08b110b2d0&hash=fd3653b7cf651710754f81f4086c5a1eec0fb69b0a88f76abc728e565f01cde0&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S1877705814003750&tid=spdf-27298b13-82a7-443b-b0ee-b8817b1005bd&sid=ffdaf21b7339d848bc8a65473cb7e8e4b2eagxrqb&type=client)\n","\n","\n","### [Paper 2 : KNN with TF-IDF Based Framework for Text Categorization](http://scholar.google.com.eg/scholar_url?url=https://www.researchgate.net/profile/Bhavna_Reddy/post/how_text_classification_is_based_on_rocchios_method/attachment/59d623706cda7b8083a1e0d1/AS:331938180157440%401456151637583/download/knn%2Bdocument%2Bclassification%2B%25281%2529.pdf&hl=en&sa=X&ei=NBGKX73dGqXGsQLNmpfoBg&scisig=AAGBfm04v_NBzeNANXyie1cThv48S6HRww&nossl=1&oi=scholarr)\n"]},{"cell_type":"markdown","metadata":{"id":"uBFjjPLy78u_"},"source":["## Agriculture:\n","### [Paper 3 : Translating climate forecasts into agricultural terms: advances and challenges](https://www.int-res.com/articles/cr2007/33/c033p027.pdf)\n","\n","### [Paper 4 :Weather analogue: a tool for real-time prediction of daily weather data realizations based on a modified k-nearest neighbor approach](https://www.sciencedirect.com/science/article/abs/pii/S1364815207001764)"]},{"cell_type":"markdown","metadata":{"id":"lF93Elgr78u_"},"source":["## Finance\n","\n","### [Paper 5 : A feature weighted support vector machine and K-nearest neighbor algorithm for stock market indices prediction](https://www.sciencedirect.com/science/article/abs/pii/S0957417417301367)\n","\n","### [Paper 6 : Multidimensional k-nearest neighbor model based on EEMD for financial time series forecasting](https://www.sciencedirect.com/science/article/abs/pii/S0378437117302091)\n"]},{"cell_type":"markdown","metadata":{"id":"J8uMHHWo78vA"},"source":["## Kaggle Competitions:\n","\n","### [Web Traffic Time Series Forecasting](https://www.kaggle.com/c/web-traffic-time-series-forecasting/discussion/39876)\n","\n","### [Facebook V: Predicting Check Ins](https://www.kaggle.com/c/facebook-v-predicting-check-ins/discussion/22083)"]},{"cell_type":"markdown","metadata":{"id":"cFBmHu4s78vD"},"source":["# Sources :\n","https://aiaspirant.com/knn-from-scratch/\n","\n","\n","https://machinelearningmastery.com/tutorial-to-implement-k-nearest-neighbors-in-python-from-scratch/\n","\n","\n","https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote02_kNN.html\n","\n","\n","http://vision.stanford.edu/teaching/cs231n-demos/knn/\n","\n","\n","https://www.quora.com/What-is-the-difference-between-a-parametric-model-and-a-non-parametric-model\n","\n","http://towardsdatascience.com/k-nearest-neighbors-and-the-curse-of-dimensionality-7d64634015d9?gi=ef7d38a06cfc"]},{"cell_type":"markdown","metadata":{"id":"mVHluOkn78vE"},"source":["**Good Luck !**\n","\n","---\n","\n"]}],"metadata":{"colab":{"provenance":[{"file_id":"1_GJJep-0RMIqE1mO3cB7iC8sbGIy9cZi","timestamp":1715439812657}]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"}},"nbformat":4,"nbformat_minor":0}