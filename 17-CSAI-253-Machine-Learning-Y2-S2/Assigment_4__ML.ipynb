{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Nural Network From Scrach**\n",
        "NN consists of:\n",
        "\n",
        "1-Layers: input layer, one {binary classification} or more {multiclass classificatio} hidden layers, and an output layer.\n",
        "\n",
        "2-Nodes: num of nodes == num of features in the MNIST . Hidden layers == variable, output layer == 10 nodes.\n",
        "\n",
        "\n",
        "Here: it is multiclass classification, thus I'll use Activation function \"RELU\" &  softmax for output layer\n"
      ],
      "metadata": {
        "id": "NTnS_3eAZ6sS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "EQEk5o98P7r2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "X_train = X_train.reshape(X_train.shape[0], -1) / 255.0\n",
        "X_test = X_test.reshape(X_test.shape[0], -1) / 255.0\n",
        "Y_train = pd.get_dummies(y_train).values.T\n",
        "Y_test = pd.get_dummies(y_test).values.T"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikrHnmMrQAfi",
        "outputId": "34d9b4be-2e22-40e6-bb08-3dc8c96ee62b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "mxsrEiUNZ4n-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NN\n",
        "input_size = X_train.shape[1]\n",
        "hidden_size = 64\n",
        "\n",
        "output_size = 10\n",
        "\n",
        "learning_rate = 0.1\n",
        "\n",
        "\n",
        "num_epochs = 2000"
      ],
      "metadata": {
        "id": "V3byx9yRQMov"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#parameters\n",
        "W1 = np.random.randn(hidden_size, input_size) * 0.01\n",
        "b1 = np.zeros((hidden_size, 1))\n",
        "W2 = np.random.randn(output_size, hidden_size) * 0.01\n",
        "b2 = np.zeros((output_size, 1))"
      ],
      "metadata": {
        "id": "NZk20uuOQUpe"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Activation function (sigmoid)\\n\",\n",
        "def sigmoid(Z):\n",
        "    return 1 / (1 + np.exp(-Z))"
      ],
      "metadata": {
        "id": "2vz0-LsfQXwv"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_propagation(X, W1, b1, W2, b2):\n",
        "    Z1 = np.dot(W1, X.T) + b1\n",
        "    A1 = sigmoid(Z1)\n",
        "    Z2 = np.dot(W2, A1) + b2\n",
        "    A2 = sigmoid(Z2)\n",
        "    return {\"Z1\": Z1, \"A1\": A1, \"Z2\": Z2, \"A2\": A2}"
      ],
      "metadata": {
        "id": "xnO3EYRJQcOf"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def backward_propagation(X, Y, cache, W1, W2):\n",
        "    m = X.shape[0]\n",
        "    A1 = cache[\"A1\"]\n",
        "    A2 = cache[\"A2\"]\n",
        "    # Calculate gradients for the second layer\n",
        "    dZ2 = A2 - Y\n",
        "    dW2 = (1 / m) * np.dot(dZ2, A1.T)\n",
        "    db2 = (1 / m) * np.sum(dZ2, axis=1, keepdims=True)\n",
        "    # Calculate gradients for the first layer\n",
        "    dZ1 = np.dot(W2.T, dZ2) * (A1 * (1 - A1))\n",
        "    dW1 = (1 / m) * np.dot(dZ1, X)\n",
        "    db1 = (1 / m) * np.sum(dZ1, axis=1, keepdims=True)\n",
        "    return {\n",
        "        \"dW1\": dW1,\n",
        "        \"db1\": db1,\n",
        "        \"dW2\": dW2,\n",
        "        \"db2\": db2\n",
        "    }\n"
      ],
      "metadata": {
        "id": "21kr_6SbQfjw"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(X_train, Y_train, W1, b1, W2, b2, num_epochs, learning_rate):\n",
        "    for epoch in range(num_epochs):\n",
        "        cache = forward_propagation(X_train, W1, b1, W2, b2)\n",
        "\n",
        "        gradients = backward_propagation(X_train, Y_train, cache, W1, W2)\n",
        "\n",
        "        # Update weights\n",
        "        W1 -= learning_rate * gradients[\"dW1\"]\n",
        "        b1 -= learning_rate * gradients[\"db1\"]\n",
        "        W2 -= learning_rate * gradients[\"dW2\"]\n",
        "        b2 -= learning_rate * gradients[\"db2\"]\n",
        "\n",
        "    return W1, b1, W2, b2\n"
      ],
      "metadata": {
        "id": "htRzRiH7QirG"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Train\n",
        "for epoch in range(num_epochs):\n",
        "    cache = forward_propagation(X_train)\n",
        "    gradients = backward_propagation(X_train, Y_train, cache)\n",
        "\n",
        "\n",
        "    W1 -= learning_rate * gradients[\"dW1\"]\n",
        "    b1 -= learning_rate * gradients[\"db1\"]\n",
        "    W2 -= learning_rate * gradients[\"dW2\"]\n",
        "    b2 -= learning_rate * gradients[\"db2\"]\n",
        "\n",
        "# Test\n",
        "cache_test = forward_propagation(X_test)\n",
        "predictions = cache_test[\"A2\"]\n",
        "y_pred = np.argmax(predictions, axis=0)\n",
        "y_true = np.argmax(Y_test, axis=0)\n",
        "\n",
        "\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "print(\"Accuracy:\", accuracy * 100, \"%\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5lxEALBWQjAQ",
        "outputId": "f328c133-4b11-4638-d8a2-77ccc7082fde"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 90.67 %\n",
            "Confusion Matrix:\n",
            "[[ 963    0    1    2    1    3    7    1    2    0]\n",
            " [   0 1105    1    4    1    1    4    1   18    0]\n",
            " [  13    6  907   17   16    1   17   12   39    4]\n",
            " [   3    1   23  907    1   26    4   19   21    5]\n",
            " [   1    5    4    0  906    1   11    2    5   47]\n",
            " [  20    3    6   51   15  731   16    9   31   10]\n",
            " [  20    3    7    1   11   17  895    1    3    0]\n",
            " [   6   20   32    2   11    0    0  929    3   25]\n",
            " [  10   11    9   26   11   25   16   10  841   15]\n",
            " [  13    8    5   12   46   10    1   25    6  883]]\n"
          ]
        }
      ]
    }
  ]
}