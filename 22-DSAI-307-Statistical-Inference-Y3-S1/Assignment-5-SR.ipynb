{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP4zz83f0C1scRtitcZ1wck"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install hmmlearn\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YKMivqKTypwM","executionInfo":{"status":"ok","timestamp":1763902677061,"user_tz":-120,"elapsed":6220,"user":{"displayName":"Abdalrahman Omer 202202254","userId":"05776575457129899449"}},"outputId":"35cc0fd5-11ba-4417-9a65-fd1834830fb6"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting hmmlearn\n","  Downloading hmmlearn-0.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n","Requirement already satisfied: numpy>=1.10 in /usr/local/lib/python3.12/dist-packages (from hmmlearn) (2.0.2)\n","Requirement already satisfied: scikit-learn!=0.22.0,>=0.16 in /usr/local/lib/python3.12/dist-packages (from hmmlearn) (1.6.1)\n","Requirement already satisfied: scipy>=0.19 in /usr/local/lib/python3.12/dist-packages (from hmmlearn) (1.16.3)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn!=0.22.0,>=0.16->hmmlearn) (1.5.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn!=0.22.0,>=0.16->hmmlearn) (3.6.0)\n","Downloading hmmlearn-0.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (165 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: hmmlearn\n","Successfully installed hmmlearn-0.3.3\n"]}]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K52bMAFzyBMO","executionInfo":{"status":"ok","timestamp":1763902821003,"user_tz":-120,"elapsed":93,"user":{"displayName":"Abdalrahman Omer 202202254","userId":"05776575457129899449"}},"outputId":"a743e853-d2a8-4761-dc35-1cf2dca8982c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Script loaded. Use --demo --timit_root PATH to run the demo pipeline.\n"]}],"source":["import os\n","import glob\n","import numpy as np\n","import librosa\n","from sklearn.model_selection import train_test_split\n","from hmmlearn.hmm import GaussianHMM\n","from scipy.stats import multivariate_normal\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n","\n","# -----------------------------\n","# CONFIG\n","# -----------------------------\n","SAMPLE_RATE = 16000   # TIMIT uses 16k\n","N_MFCC = 13\n","HMM_COV_TYPE = 'diag'  # 'diag' or 'full'\n","RANDOM_STATE = 42\n","\n","# -----------------------------\n","# HELPERS: Data loading & feature extraction\n","# -----------------------------\n","def load_phn_file(phn_path):\n","    \"\"\"\n","    Parse a TIMIT .phn file. .phn lines: <start_sample> <end_sample> <phoneme_label>\n","    Returns list of tuples (start_sample, end_sample, phoneme_label)\n","    \"\"\"\n","    items = []\n","    with open(phn_path, 'r') as f:\n","        for line in f:\n","            parts = line.strip().split()\n","            if len(parts) != 3:\n","                continue\n","            s, e, p = int(parts[0]), int(parts[1]), parts[2]\n","            items.append((s, e, p))\n","    return items\n","\n","def extract_mfcc(wav_path, sr=SAMPLE_RATE, n_mfcc=N_MFCC, hop_length=160, n_fft=400):\n","    \"\"\"\n","    Load wav and return MFCC feature matrix (T x n_mfcc)\n","    \"\"\"\n","    x, sr_loaded = librosa.load(wav_path, sr=sr)\n","    mfcc = librosa.feature.mfcc(x, sr=sr, n_mfcc=n_mfcc, n_fft=n_fft, hop_length=hop_length)\n","    # transpose to (T, n_mfcc)\n","    return mfcc.T\n","\n","def load_timit_word_files(timit_root, words_list=None, max_per_word=None):\n","    \"\"\"\n","    Load TIMIT where each WAV has a .phn. Return dict:\n","      {word: [(wav_path, phn_path), ...]}\n","    This function assumes filename conventions: audio .WAV and .PHN with same basename.\n","    Customize to your layout if necessary.\n","    \"\"\"\n","    pairs = {}\n","    # Search for all .wav files recursively\n","    wav_files = glob.glob(os.path.join(timit_root, '**', '*.wav'), recursive=True)\n","    for wav in wav_files:\n","        phn = os.path.splitext(wav)[0] + '.phn'\n","        if not os.path.exists(phn):\n","            continue\n","        # Try to find the word label for this file. TIMIT sentences contain words.\n","        # For this assignment, user likely pre-split each file to single word; otherwise you'd need to parse transcripts.\n","        # We'll use parent folder name as a proxy label if words_list not provided.\n","        word_label = os.path.basename(os.path.dirname(wav))\n","        if words_list is not None and word_label not in words_list:\n","            continue\n","        pairs.setdefault(word_label, []).append((wav, phn))\n","    if max_per_word:\n","        for w in list(pairs.keys()):\n","            pairs[w] = pairs[w][:max_per_word]\n","    return pairs\n","\n","# -----------------------------\n","# HMM training utilities\n","# -----------------------------\n","def make_obs_list_for_word(pairs, feature_extractor=extract_mfcc):\n","    \"\"\"\n","    For a list of (wav, phn) pairs, return list of observation arrays (T_i x D)\n","    \"\"\"\n","    obs = []\n","    for wav, phn in pairs:\n","        feat = feature_extractor(wav)  # (T, D)\n","        obs.append(feat)\n","    return obs\n","\n","def train_gaussian_hmm(observations, n_components=5, cov_type=HMM_COV_TYPE, n_iter=100):\n","    \"\"\"\n","    Train a GaussianHMM on a list of observation sequences (list of arrays T_i x D).\n","    Returns trained model.\n","    \"\"\"\n","    # hmmlearn expects concatenated data + lengths\n","    X = np.vstack(observations)\n","    lengths = [o.shape[0] for o in observations]\n","    model = GaussianHMM(n_components=n_components, covariance_type=cov_type,\n","                        n_iter=n_iter, random_state=RANDOM_STATE, verbose=False)\n","    model.fit(X, lengths)\n","    return model\n","\n","# -----------------------------\n","# Emission log-probabilities (for Gaussian HMM states)\n","# -----------------------------\n","def emission_logprob(obs, means, covars, cov_type='diag'):\n","    \"\"\"\n","    Compute emission log-probabilities:\n","      obs: (T, D)\n","      means: (n_states, D)\n","      covars: depends on cov_type:\n","        - diag: (n_states, D)\n","        - full: (n_states, D, D)\n","    Returns log_B: (T, n_states) where log_B[t, j] = log P(obs[t] | state=j)\n","    \"\"\"\n","    T = obs.shape[0]\n","    n_states = means.shape[0]\n","    D = obs.shape[1]\n","    log_B = np.zeros((T, n_states))\n","    for j in range(n_states):\n","        mu = means[j]\n","        if cov_type == 'diag':\n","            cov = np.diag(covars[j]) if covars.shape[1] == D else np.diag(covars[j])\n","            # use multivariate_normal with cov\n","            rv = multivariate_normal(mean=mu, cov=np.diag(covars[j]), allow_singular=True)\n","        else:  # 'full'\n","            rv = multivariate_normal(mean=mu, cov=covars[j], allow_singular=True)\n","        # compute logpdf for all timesteps\n","        log_B[:, j] = rv.logpdf(obs)\n","    return log_B\n","\n","# -----------------------------\n","# Your own Forward algorithm (log-space)\n","# -----------------------------\n","def forward_log(start_logprob, trans_logprob, log_emlik):\n","    \"\"\"\n","    Log-forward algorithm (alpha in log-space).\n","    start_logprob: (n_states,)\n","    trans_logprob: (n_states, n_states) -- log of transition matrix\n","    log_emlik: (T, n_states) -- log emission likelihoods per timestep\n","    Returns:\n","      alpha_logs: (T, n_states), log_likelihood (scalar)\n","    \"\"\"\n","    T, n_states = log_emlik.shape\n","    alpha = np.full((T, n_states), -np.inf)\n","    # init\n","    alpha[0] = start_logprob + log_emlik[0]\n","    for t in range(1, T):\n","        # For numeric stability use log-sum-exp\n","        prev = alpha[t-1]  # (n_states,)\n","        # compute next alpha[t, j] = logsum_k ( alpha[t-1,k] + trans_log[k,j] ) + log_emlik[t, j]\n","        for j in range(n_states):\n","            tmp = prev + trans_logprob[:, j]  # shape (n_states,)\n","            # log-sum-exp:\n","            m = np.max(tmp)\n","            if np.isfinite(m):\n","                alpha[t, j] = np.log(np.sum(np.exp(tmp - m))) + m + log_emlik[t, j]\n","            else:\n","                alpha[t, j] = -np.inf\n","    # log-likelihood = log-sum-exp over alpha[T-1]\n","    m = np.max(alpha[-1])\n","    if np.isfinite(m):\n","        loglik = np.log(np.sum(np.exp(alpha[-1] - m))) + m\n","    else:\n","        loglik = -np.inf\n","    return alpha, loglik\n","\n","# -----------------------------\n","# Your own Viterbi algorithm (log-space)\n","# -----------------------------\n","def viterbi_log(start_logprob, trans_logprob, log_emlik):\n","    \"\"\"\n","    Log-Viterbi to compute most likely state path.\n","    Returns path (length T) and path log-prob.\n","    \"\"\"\n","    T, n_states = log_emlik.shape\n","    delta = np.full((T, n_states), -np.inf)  # best score up to t in state j\n","    psi = np.zeros((T, n_states), dtype=int)  # backpointers\n","\n","    delta[0] = start_logprob + log_emlik[0]\n","    psi[0] = -1\n","    for t in range(1, T):\n","        for j in range(n_states):\n","            scores = delta[t-1] + trans_logprob[:, j]  # candidate scores\n","            best_prev = np.argmax(scores)\n","            delta[t, j] = scores[best_prev] + log_emlik[t, j]\n","            psi[t, j] = best_prev\n","\n","    # backtrack\n","    path = np.zeros(T, dtype=int)\n","    path[T-1] = np.argmax(delta[T-1])\n","    for t in reversed(range(1, T)):\n","        path[t-1] = psi[t, path[t]]\n","    # path score\n","    path_score = delta[T-1, path[T-1]]\n","    return path, path_score\n","\n","# -----------------------------\n","# Utilities for converting to log-space and using model params\n","# -----------------------------\n","def model_to_log_params(model):\n","    \"\"\"\n","    Extract model parameters and convert to log-space\n","    \"\"\"\n","    startprob = model.startprob_\n","    trans = model.transmat_\n","    start_log = np.log(startprob + 1e-300)\n","    trans_log = np.log(trans + 1e-300)\n","    means = model.means_  # (n_states, D)\n","    covars = model.covars_\n","    cov_type = model.covariance_type\n","    return start_log, trans_log, means, covars, cov_type\n","\n","# -----------------------------\n","# Example pipeline\n","# -----------------------------\n","def example_pipeline(timit_root, words_to_model=None, max_files_per_word=20, n_hmm_states=5):\n","    \"\"\"\n","    Full example pipeline:\n","     - Loads data for a small set of words (or directory labels)\n","     - Extracts MFCCs\n","     - Trains one GaussianHMM per word\n","     - Inspects the model\n","     - Runs own Forward and Viterbi on a held-out test sample\n","    \"\"\"\n","    print(\"Scanning dataset...\")\n","    pairs_by_word = load_timit_word_files(timit_root, words_list=words_to_model, max_per_word=max_files_per_word)\n","    models = {}\n","    for word, pairs in pairs_by_word.items():\n","        print(f\"\\nWord: {word}  |  files: {len(pairs)}\")\n","        if len(pairs) < 2:\n","            print(\"  - Not enough data, skipping.\")\n","            continue\n","        obs = make_obs_list_for_word(pairs)\n","        # train-test split for demonstration\n","        obs_train, obs_test = train_test_split(obs, test_size=0.2, random_state=RANDOM_STATE)\n","        n_states = n_hmm_states\n","        print(f\"  - Training HMM with n_components={n_states}\")\n","        model = train_gaussian_hmm(obs_train, n_components=n_states)\n","        models[word] = (model, obs_test)\n","        # Inspect\n","        print(\"  startprob shape:\", model.startprob_.shape)\n","        print(\"  transmat shape:\", model.transmat_.shape)\n","        print(\"  means shape:\", model.means_.shape)\n","        print(\"  covars shape:\", model.covars_.shape)\n","    # demo evaluation on a single test sample for first model\n","    if models:\n","        demo_word = next(iter(models.keys()))\n","        model, obs_test_list = models[demo_word]\n","        test_obs = obs_test_list[0]\n","        print(f\"\\nDemo on word '{demo_word}' test sample. Observations shape: {test_obs.shape}\")\n","\n","        # compute emission logprobs\n","        start_log, trans_log, means, covars, cov_type = model_to_log_params(model)\n","        log_eml = emission_logprob(test_obs, means, covars, cov_type)\n","        alpha, loglik = forward_log(start_log, trans_log, log_eml)\n","        path, path_score = viterbi_log(start_log, trans_log, log_eml)\n","\n","        print(f\"Forward log-likelihood: {loglik:.3f}\")\n","        print(f\"Viterbi path length: {len(path)}  Path score (log): {path_score:.3f}\")\n","        print(\"Viterbi state sequence (first 20):\", path[:20])\n","    else:\n","        print(\"No models trained. Check your dataset path and layout.\")\n","\n","    return models\n","\n","# -----------------------------\n","# Main guard for script\n","# -----------------------------\n","if __name__ == \"__main__\":\n","    import argparse\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument(\"--timit_root\", type=str, required=False,\n","                        help=\"Path to TIMIT root (or any directory with .wav and .phn pairs).\")\n","    parser.add_argument(\"--demo\", action=\"store_true\", help=\"Run demo pipeline (requires timit_root).\")\n","    parser.add_argument(\"--n_states\", type=int, default=7, help=\"HMM states per model\")\n","    args = parser.parse_known_args()[0]\n","\n","    if args.demo:\n","        if args.timit_root is None:\n","            print(\"Provide --timit_root path where .wav and .phn are located.\")\n","        else:\n","            example_pipeline(args.timit_root, max_files_per_word=30, n_hmm_states=args.n_states)\n","    else:\n","        print(\"Script loaded. Use --demo --timit_root PATH to run the demo pipeline.\")"]},{"cell_type":"code","source":[],"metadata":{"id":"M627QNgSyXR5"},"execution_count":null,"outputs":[]}]}