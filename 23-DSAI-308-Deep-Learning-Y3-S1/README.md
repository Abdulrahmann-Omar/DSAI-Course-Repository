# DSAI 308 - Deep Learning (Y3-S1)

<div align="center">

**Fall 2023** | **Advanced AI Course** | **Research Component**

[![Materials](https://img.shields.io/badge/Materials-Complete-success)](.)
[![Projects](https://img.shields.io/badge/Projects-Multiple-blue)](.)
[![Skills](https://img.shields.io/badge/Skills-CNNs%20|%20RNNs%20|%20Transformers-orange)](.)

</div>

---

## ğŸ“‹ Course Overview

Advanced deep learning covering neural network architectures, training techniques, and modern applications. Emphasis on both theoretical understanding and practical implementation of state-of-the-art models using PyTorch.

---

## ğŸ¯ Learning Outcomes Achieved

### Neural Network Fundamentals
- âœ… Backpropagation and automatic differentiation
- âœ… Optimization algorithms (SGD, Adam, RMSprop)
- âœ… Regularization techniques (Dropout, Batch Norm)
- âœ… Learning rate scheduling

### Advanced Architectures
- âœ… Convolutional Neural Networks (ResNet, DenseNet)
- âœ… Recurrent Neural Networks (LSTM, GRU)
- âœ… Transformer architecture
- âœ… Generative models (VAE, GAN)

### Practical Skills
- âœ… PyTorch implementation from scratch
- âœ… Custom training loops
- âœ… Transfer learning and fine-tuning
- âœ… Model debugging and optimization

---

## ğŸ“š Topics Covered

### Module 1: Foundations (Weeks 1-3)
- Neural network fundamentals
- Backpropagation algorithm
- Activation functions
- Loss functions and optimization

### Module 2: CNNs (Weeks 4-6)
- Convolutional operations
- Classic architectures (AlexNet, VGG)
- Modern architectures (ResNet, EfficientNet)
- Object detection and segmentation

### Module 3: Sequence Models (Weeks 7-9)
- RNNs and vanishing gradients
- LSTM and GRU
- Seq2Seq models
- Attention mechanisms

### Module 4: Transformers & Genertic Models (Weeks 10-14)
- Self-attention and Transformers
- BERT and GPT overview
- Variational Autoencoders
- Generative Adversarial Networks

---

## ğŸ’» Technical Projects

### Project 1: Image Classification System
**Objective**: Build and optimize CNN for image classification

**Implementation**:
- Custom ResNet implementation
- Data augmentation strategies
- Hyperparameter optimization
- Performance analysis

[ğŸ“‚ GitHub Repository](https://github.com/Abdulrahmann-Omar)

---

### Project 2: Sequence-to-Sequence Model
**Objective**: Implement attention-based seq2seq model

**Implementation**:
- Encoder-decoder architecture
- Attention mechanism
- Beam search decoding
- BLEU score evaluation

[ğŸ“‚ GitHub Repository](https://github.com/Abdulrahmann-Omar)

**Technologies**: PyTorch, torchvision, TensorBoard, Weights & Biases

---

## ğŸ“– Key Resources

### Textbooks
- **"Deep Learning"** - Goodfellow, Bengio, Courville
- **"Dive into Deep Learning"** - Zhang et al.

### Online Courses
- CS231n: Convolutional Neural Networks (Stanford)
- CS224n: Natural Language Processing (Stanford)

---

## ğŸ¯ Course Impact on Graduate Studies Preparation

Deep learning course provided:
- Strong foundation in modern architectures
- Implementation skills from scratch
- Research-ready experiment design
- Foundation for advanced research

---

<div align="center">

**Key Takeaway**: Understanding deep learning from first principles enables innovation beyond existing architectures.

[â¬…ï¸ Back to All Courses](../README.md#-complete-course-catalog)

</div>
