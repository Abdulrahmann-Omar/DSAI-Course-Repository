{"cells":[{"cell_type":"markdown","metadata":{"id":"O_BFRBf1YDSd"},"source":["# üö® 3-HOUR MC & TD CRASH COURSE ‚Äî EXAM FOCUSED üö®\n","\n","> **You have 3 hours. No lectures attended. This is your survival guide.**\n","\n","---\n","\n","## ‚è∞ YOUR 3-HOUR BATTLE PLAN\n","\n","| Time | What to Do |\n","|------|------------|\n","| **Hour 1** | Read concepts + Copy & Run MC code |\n","| **Hour 2** | Read TD section + Practice TD code |\n","| **Hour 3** | Full exam simulation |\n","\n","---\n","\n","## üîπ Model-Free RL (30 sec)\n","\n","- **Model-Based** (DP): You KNOW the environment\n","- **Model-Free** (MC & TD): You DON'T know. You LEARN by trying!\n","\n","---\n","\n","## üîπ Monte Carlo (MC)\n","\n","> **MC = Learn from COMPLETE episodes**\n","\n","### Return Formula:\n","$$G_t = R_{t+1} + \\gamma \\cdot G_{t+1}$$\n","\n","**Calculate G BACKWARDS!**\n","\n","---\n","\n","## üîπ First-Visit vs Every-Visit\n","\n","| Method | What to do |\n","|--------|------------|\n","| **First-Visit** | Use `visited` set, update first occurrence only |\n","| **Every-Visit** | NO `visited` set, update every occurrence |\n","\n","---\n","\n","## üîπ Temporal Difference (TD)\n","\n","> **TD = Learn DURING the journey, not after**\n","\n","### TD Update Formula:\n","$$V(S_t) \\leftarrow V(S_t) + \\alpha \\left[ R_{t+1} + \\gamma V(S_{t+1}) - V(S_t) \\right]$$\n","\n","| MC | TD |\n","|----|----|  \n","| Update AFTER episode | Update EVERY step |\n","| Uses actual G | Uses estimated return |"]},{"cell_type":"markdown","metadata":{"id":"swNxBQOqYDSx"},"source":["---\n","# üíª PART 1: SETUP & CONSTANTS"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"mUEdNnX7YDSy","executionInfo":{"status":"ok","timestamp":1765845714228,"user_tz":-120,"elapsed":7,"user":{"displayName":"Abdalrahman Omer 202202254","userId":"05776575457129899449"}}},"outputs":[],"source":["import random\n","import matplotlib.pyplot as plt\n","\n","# Constants\n","GRID_SIZE = 5\n","REWARD_GOAL = 10\n","REWARD_STEP = -1"]},{"cell_type":"markdown","metadata":{"id":"SqztXew4YDS0"},"source":["---\n","# üìù Task 1: Generate Episode"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3SHkfP34YDS2","executionInfo":{"status":"ok","timestamp":1765845714323,"user_tz":-120,"elapsed":68,"user":{"displayName":"Abdalrahman Omer 202202254","userId":"05776575457129899449"}},"outputId":"20c41893-a57e-42b5-91a5-87c9b2f3bc19"},"outputs":[{"output_type":"stream","name":"stdout","text":["Episode: [((4, 0), -1), ((3, 0), -1), ((3, 0), -1), ((3, 0), -1), ((3, 0), -1), ((2, 0), -1), ((3, 0), -1), ((2, 0), -1), ((2, 0), -1), ((1, 0), -1), ((2, 0), -1), ((2, 0), -1), ((3, 0), -1), ((3, 1), -1), ((2, 1), -1), ((2, 2), -1), ((2, 1), -1), ((2, 2), -1), ((1, 2), -1), ((1, 1), -1)]\n","Length: 20\n"]}],"source":["def generate_episode():\n","    \"\"\"\n","    Generate an episode with random start and goal at corners.\n","    Returns: list of (state, reward) tuples\n","    \"\"\"\n","    # Random start at corners\n","    agent_x = random.choice([0, GRID_SIZE-1])\n","    agent_y = random.choice([0, GRID_SIZE-1])\n","\n","    # Random goal at corners\n","    goal_x = random.choice([0, GRID_SIZE-1])\n","    goal_y = random.choice([0, GRID_SIZE-1])\n","\n","    episode = []\n","    steps = 0\n","\n","    while (agent_x, agent_y) != (goal_x, goal_y) and steps < 20:\n","        state = (agent_x, agent_y)  # MUST be tuple!\n","\n","        # Random action: 0=left, 1=up, 2=right, 3=down\n","        action = random.choice([0, 1, 2, 3])\n","\n","        # Move (stay in grid boundaries!)\n","        if action == 0:    # left\n","            agent_x = max(agent_x - 1, 0)\n","        elif action == 1:  # up\n","            agent_y = max(agent_y - 1, 0)\n","        elif action == 2:  # right\n","            agent_x = min(agent_x + 1, GRID_SIZE - 1)\n","        elif action == 3:  # down\n","            agent_y = min(agent_y + 1, GRID_SIZE - 1)\n","\n","        # Reward\n","        if (agent_x, agent_y) == (goal_x, goal_y):\n","            reward = REWARD_GOAL\n","        else:\n","            reward = REWARD_STEP\n","\n","        episode.append((state, reward))\n","        steps += 1\n","\n","    return episode\n","\n","# TEST:\n","ep = generate_episode()\n","print(f\"Episode: {ep}\")\n","print(f\"Length: {len(ep)}\")"]},{"cell_type":"markdown","metadata":{"id":"5eJBMv3_YDS9"},"source":["---\n","# üìù Task 2: First-Visit MC Update"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U4BjlIxnYDS-","executionInfo":{"status":"ok","timestamp":1765846222324,"user_tz":-120,"elapsed":91,"user":{"displayName":"Abdalrahman Omer 202202254","userId":"05776575457129899449"}},"outputId":"d191fae2-9fd0-438b-ba94-dc7ff088688f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Learned 25 states\n","{(3, 0): -915.4273991850314, (4, 0): -1190.4530215842558, (4, 1): -869.1483171781962, (3, 1): -1113.9355449857514, (4, 2): -803.2210257882847, (2, 4): -851.7449720015401, (3, 4): -805.4751014570975, (1, 4): -961.0845299017128, (0, 4): -1262.2949418132814, (0, 3): -893.1919640006225, (0, 2): -905.768727563359, (1, 2): -1072.8466951198761, (0, 1): -1031.9882734826126, (2, 2): -944.4376927091021, (2, 1): -1104.9348260246638, (2, 0): -922.3648735075295, (1, 1): -1176.7985242535701, (0, 0): -1450.95971148657, (3, 2): -1009.3774852316652, (3, 3): -1052.508613656553, (4, 3): -890.360000114809, (1, 0): -1097.8603513095545, (1, 3): -1024.622574288318, (2, 3): -1111.0170212925282, (4, 4): -1142.6692508717683}\n"]}],"source":["def first_visit_mc(episode, V, gamma=0.9):\n","    \"\"\"\n","    First-visit Monte Carlo update.\n","\n","    Args:\n","        episode: list of (state, reward)\n","        V: dict mapping state -> value\n","        gamma: discount factor\n","    \"\"\"\n","    # Extract states and rewards\n","    states = [s for (s, r) in episode]\n","    rewards = [r for (s, r) in episode]\n","\n","    G = 0.0\n","    visited = set()  # Track first visits!\n","\n","    # Go BACKWARDS through episode\n","    for t in reversed(range(len(episode))):\n","        state = states[t]\n","        reward = rewards[t]\n","\n","        # Calculate return\n","        G = gamma * G + reward\n","\n","        # First-visit check\n","        if state not in visited:\n","            visited.add(state)\n","\n","            # Update value\n","            old_value = V.get(state, 0.0)\n","            V[state] = old_value + G\n","\n","    return V\n","  # RUN First-Visit MC:\n","V = {}\n","for _ in range(1000):\n","    episode = generate_episode()\n","    V = first_visit_mc(episode, V)\n","\n","print(f\"Learned {len(V)} states\")\n","print(V)"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"gNTFxSP8YDS_","executionInfo":{"status":"ok","timestamp":1765846223904,"user_tz":-120,"elapsed":11,"user":{"displayName":"Abdalrahman Omer 202202254","userId":"05776575457129899449"}}},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"GMZGX9pNYDTA"},"source":["---\n","# üìù Task 3: Every-Visit MC Update\n","\n","**ONLY DIFFERENCE: No `visited` set!**"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"3I3KlLQaYDTB","executionInfo":{"status":"ok","timestamp":1765845714410,"user_tz":-120,"elapsed":8,"user":{"displayName":"Abdalrahman Omer 202202254","userId":"05776575457129899449"}}},"outputs":[],"source":["def every_visit_mc(episode, V, gamma=0.9):\n","    \"\"\"\n","    Every-visit Monte Carlo update.\n","    ONLY DIFFERENCE: No 'visited' set!\n","    \"\"\"\n","    states = [s for (s, r) in episode]\n","    rewards = [r for (s, r) in episode]\n","\n","    G = 0.0\n","    # NO visited set here!\n","\n","    for t in reversed(range(len(episode))):\n","        state = states[t]\n","        reward = rewards[t]\n","\n","        G = gamma * G + reward\n","\n","        # Update EVERY time (no first-visit check)\n","        old_value = V.get(state, 0.0)\n","        V[state] = old_value + G\n","\n","    return V"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jh460syuYDTD","executionInfo":{"status":"ok","timestamp":1765845714445,"user_tz":-120,"elapsed":30,"user":{"displayName":"Abdalrahman Omer 202202254","userId":"05776575457129899449"}},"outputId":"684485aa-5a4b-4aff-c670-3b64dadef91f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Every-visit learned 25 states\n","{(4, 0): -5255.50089938093, (3, 0): -3114.4808273069757, (3, 1): -2446.818135748256, (2, 1): -2569.181179169013, (3, 2): -2354.5685570368505, (4, 1): -3127.8520147930158, (1, 1): -2884.4997832420995, (0, 1): -3322.869151701406, (0, 0): -6385.123414392148, (3, 3): -2296.8130032230856, (3, 4): -2537.4294759057825, (4, 4): -5319.054021994446, (4, 3): -3067.215912752964, (4, 2): -2776.817307994262, (1, 4): -2967.248151001546, (2, 4): -1900.7062327873834, (2, 3): -1985.6616169236022, (1, 3): -2491.7295788956358, (0, 3): -3194.8875975090223, (0, 2): -2465.513648754507, (1, 2): -2411.831999791466, (2, 2): -2281.240469602955, (0, 4): -5850.140370460864, (1, 0): -3216.649870807307, (2, 0): -2606.2171452983116}\n"]}],"source":["# RUN Every-Visit MC:\n","V2 = {}\n","for _ in range(1000):\n","    episode = generate_episode()\n","    V2 = every_visit_mc(episode, V2)\n","\n","print(f\"Every-visit learned {len(V2)} states\")\n","print(V2)"]},{"cell_type":"markdown","metadata":{"id":"pAiHSONnYDTE"},"source":["---\n","# üìù Task 4: TD(0) Update\n","\n","**Key Formula:** `V[s] = V[s] + alpha √ó (reward + gamma √ó V[s'] - V[s])`"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"j1gXnnGXYDTE","executionInfo":{"status":"ok","timestamp":1765845714474,"user_tz":-120,"elapsed":26,"user":{"displayName":"Abdalrahman Omer 202202254","userId":"05776575457129899449"}}},"outputs":[],"source":["def td_update(V, state, reward, next_state, alpha=0.5, gamma=0.9, terminal=False):\n","    \"\"\"\n","    TD(0) update for a single step.\n","    \"\"\"\n","    old_value = V.get(state, 0.0)\n","\n","    if terminal:\n","        next_value = 0\n","    else:\n","        next_value = V.get(next_state, 0.0)\n","\n","    # TD update\n","    td_error = reward + gamma * next_value - old_value\n","    V[state] = old_value + alpha * td_error\n","\n","    return V\n","\n","\n","def run_td_episode(V, alpha=0.5, gamma=0.9):\n","    \"\"\"Run one episode with TD updates.\"\"\"\n","    agent_x = random.choice([0, GRID_SIZE-1])\n","    agent_y = random.choice([0, GRID_SIZE-1])\n","    goal_x = random.choice([0, GRID_SIZE-1])\n","    goal_y = random.choice([0, GRID_SIZE-1])\n","\n","    steps = 0\n","\n","    while (agent_x, agent_y) != (goal_x, goal_y) and steps < 20:\n","        state = (agent_x, agent_y)\n","\n","        action = random.choice([0, 1, 2, 3])\n","\n","        if action == 0:\n","            agent_x = max(agent_x - 1, 0)\n","        elif action == 1:\n","            agent_y = max(agent_y - 1, 0)\n","        elif action == 2:\n","            agent_x = min(agent_x + 1, GRID_SIZE - 1)\n","        elif action == 3:\n","            agent_y = min(agent_y + 1, GRID_SIZE - 1)\n","\n","        next_state = (agent_x, agent_y)\n","\n","        if next_state == (goal_x, goal_y):\n","            reward = REWARD_GOAL\n","            terminal = True\n","        else:\n","            reward = REWARD_STEP\n","            terminal = False\n","\n","        # TD UPDATE HAPPENS HERE (during episode!)\n","        V = td_update(V, state, reward, next_state, alpha, gamma, terminal)\n","\n","        steps += 1\n","\n","    return V\n","# RUN TD:\n","V_td = {}\n","for _ in range(1000):\n","    V_td = run_td_episode(V_td)\n","\n","print(f\"TD learned {len(V_td)} states\")\n","print(V_td)"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cilc-DCXYDTH","executionInfo":{"status":"ok","timestamp":1765845714519,"user_tz":-120,"elapsed":41,"user":{"displayName":"Abdalrahman Omer 202202254","userId":"05776575457129899449"}},"outputId":"5930ca78-c69e-4939-8f72-20bad127a615"},"outputs":[{"output_type":"stream","name":"stdout","text":["TD learned 25 states\n","{(0, 4): -9.542105967395653, (1, 4): 0.289157803461352, (1, 3): -9.470822127613678, (1, 2): -9.457766609015643, (2, 2): -9.339372955084993, (1, 1): -9.573580999335814, (2, 1): -8.50523568197342, (4, 0): -4.549163341699257, (3, 0): -9.22103575864789, (3, 1): -8.19510648293571, (3, 2): -8.601840518577443, (2, 0): -9.51017563623643, (4, 1): -3.9366260834089326, (2, 3): -9.474048482229826, (4, 2): -9.02236038036385, (4, 3): -9.422822045083143, (4, 4): -9.41479623805094, (3, 3): -9.436408202236734, (3, 4): -9.397681778392851, (2, 4): -9.367639220374532, (0, 0): -9.644435942612116, (1, 0): -9.580124600057573, (0, 1): -9.655005831410275, (0, 3): -9.593441302715934, (0, 2): -9.661727716602858}\n"]}],"source":[]},{"cell_type":"markdown","metadata":{"id":"yoUPX5naYDTM"},"source":["---\n","# ‚ùå COMMON MISTAKES\n","\n","### 1. Wrong Loop Direction\n","```python\n","# ‚ùå WRONG:\n","for t in range(len(episode)):\n","\n","# ‚úÖ RIGHT:\n","for t in reversed(range(len(episode))):\n","```\n","\n","### 2. First-Visit Check\n","```python\n","# ‚ùå WRONG:\n","if state not in V:\n","\n","# ‚úÖ RIGHT:\n","if state not in visited:\n","    visited.add(state)\n","```\n","\n","### 3. Boundaries\n","```python\n","# ‚ùå WRONG:\n","agent_x = min(agent_x + 1, GRID_SIZE)\n","\n","# ‚úÖ RIGHT:\n","agent_x = min(agent_x + 1, GRID_SIZE - 1)\n","```\n","\n","---\n","\n","# üéØ FORMULAS RECAP\n","\n","**MC Return:** `G = reward + gamma √ó (next G)` ‚Äî GO BACKWARDS!\n","\n","**TD Update:** `V[s] = V[s] + alpha √ó (reward + gamma √ó V[s'] - V[s])` ‚Äî UPDATE EVERY STEP!\n","\n","**First-Visit:** uses `visited` set | **Every-Visit:** no `visited` set"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.12.0"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}