# -*- coding: utf-8 -*-
"""Another copy of IR Final Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aDmfzSHpQ0_6thwdy9Go0CIDOGpgIGfU

##Pip installs
"""

!pip install nltk
import nltk
nltk.download('punkt_tab')

!pip install readability-lxml

!pip install pyterrier

!pip install --upgrade python-terrier

!pip install gradio

"""##Importing"""

#!/usr/bin/env python3
# Smart Information Retrieval System
# A comprehensive IR system with web crawling, preprocessing, and search capabilities
# Designed for Google Colab execution

# ----- IMPORTS AND SETUP -----
import os
import time
import random
import re
import pickle
import numpy as np
import pandas as pd
from PIL import Image
from urllib.parse import urlparse, urljoin, urlunparse
from collections import Counter
import requests
from bs4 import BeautifulSoup
from readability import Document

# NLP and IR imports
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer





# Import PyTerrier for IR models
import pyterrier as pt
if not pt.started():
    pt.init()

# Import Gradio for GUI
import gradio as gr

"""#test"""

#!/usr/bin/env python3
# Smart Information Retrieval System
# A comprehensive IR system with web crawling, preprocessing, and search capabilities
# Designed for Google Colab execution

# ----- IMPORTS AND SETUP -----
import os
import time
import random
import re
import pickle
import numpy as np
import pandas as pd
from PIL import Image
from urllib.parse import urlparse, urljoin, urlunparse
from collections import Counter
import requests
from bs4 import BeautifulSoup
from readability import Document

# NLP and IR imports
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer

# Import PyTerrier for IR models
import pyterrier as pt
if not pt.started():
    pt.init()

# Import Gradio for GUI
import gradio as gr

# ----- DATA COLLECTION / WEB CRAWLER FUNCTIONS -----

def is_image(url):
    """Check if a URL points to an image file based on extension"""
    return url.lower().endswith(('.jpg', '.jpeg', '.png', '.gif', '.bmp', '.webp'))

def get_robots_rules(domain):
    """Fetch and parse robots.txt rules for a domain"""
    try:
        robots_url = f"https://{domain}/robots.txt"
        response = requests.get(robots_url, timeout=5)
        if response.status_code == 200:
            # Simple parsing - this could be enhanced with a proper robots parser
            disallowed = []
            for line in response.text.split('\n'):
                if line.lower().startswith('disallow:'):
                    path = line.split(':', 1)[1].strip()
                    if path:
                        disallowed.append(path)
            return disallowed
        return []
    except:
        return []

def is_allowed(url, disallowed_paths):
    """Check if crawling a URL is allowed based on robots.txt rules"""
    parsed = urlparse(url)
    path = parsed.path

    for disallowed in disallowed_paths:
        if path.startswith(disallowed):
            return False
    return True

def extract_readable_content(html):
    """Use readability to extract the main content from HTML"""
    try:
        doc = Document(html)
        summary_html = doc.summary()
        title = doc.title()
        content = BeautifulSoup(summary_html, "html.parser").get_text(separator=' ', strip=True)
        return title, content
    except:
        return None, None

def crawl(url, depth=2, delay_range=(1, 3), data=None, visited=None, visited_data=None, ids=None):
    """
    Web crawler function that respects robots.txt and extracts content

    Args:
        url: Starting URL to crawl
        depth: How many levels deep to crawl
        delay_range: Tuple of (min, max) seconds to delay between requests
        data: List to store crawled data
        visited: Set of visited URLs
        visited_data: Set of unique content URLs added to data
        ids: Current ID counter

    Returns:
        Tuple of (data, visited, visited_data, ids)
    """
    # Initialize data structures if not provided
    if data is None:
        data = []
    if visited is None:
        visited = set()
    if visited_data is None:
        visited_data = set()
    if ids is None:
        ids = -1

    # Parse domain for robots.txt
    parsed_url = urlparse(url)
    domain = parsed_url.netloc
    disallowed_paths = get_robots_rules(domain)

    if depth == 0 or url in visited or not is_allowed(url, disallowed_paths):
        return data, visited, visited_data, ids

    # Add politeness delay
    time.sleep(random.uniform(delay_range[0], delay_range[1]))

    try:
        # Make request with timeout and user agent
        headers = {
            'User-Agent': 'SmartIRBot/1.0 (Educational Project)'
        }
        response = requests.get(url, timeout=10, headers=headers)

        if response.status_code != 200:
            return data, visited, visited_data, ids

        visited.add(url)
        soup = BeautifulSoup(response.text, "html.parser")

        # Process images
        for img in soup.find_all("img"):
            img_src = img.get("src")
            title = img.get("alt")
            if img_src and title:
                parsed_img_url = urlparse(img_src)
                img_src = urlunparse((parsed_img_url.scheme, parsed_img_url.netloc, parsed_img_url.path, '', '', ''))
                img_src = urljoin(url, img_src)
                if img_src.startswith('https') and is_image(img_src):
                    if img_src not in visited_data:
                        ids += 1
                        data.append([ids, title, img_src, None, 'image'])
                        visited_data.add(img_src)

        # Try to extract main content using readability
        title, main_content = extract_readable_content(response.text)

        # If readability fails, fallback to basic extraction
        if not main_content:
            # Remove scripts and styles
            for script in soup(["script", "style", "noscript"]):
                script.decompose()
            main_content = soup.get_text(separator=' ', strip=True)
            title = soup.title.string if soup.title else url

        # Store page content
        if main_content and title:
            ids += 1
            data.append([ids, title, url, main_content, 'html'])

        # Find links for further crawling
        for link in soup.find_all('a', href=True):
            next_url = urljoin(url, link['href'])
            # Only follow https links from same domain to be polite
            if (next_url.startswith("https") and
                urlparse(next_url).netloc == domain and
                next_url not in visited):
                data, visited, visited_data, ids = crawl(
                    next_url, depth - 1, delay_range, data, visited, visited_data, ids
                )

    except Exception as e:
        print(f"Error crawling {url}: {str(e)}")

    return data, visited, visited_data, ids

def crawl_multiple_sites(urls_with_depth, delay_range=(1, 3)):
    """
    Crawl multiple websites with specified depths

    Args:
        urls_with_depth: List of tuples (url, depth)
        delay_range: Tuple of (min, max) seconds to delay between requests

    Returns:
        DataFrame with collected data
    """
    data = []
    visited = set()
    visited_data = set()
    ids = -1

    for url, depth in urls_with_depth:
        data, visited, visited_data, ids = crawl(
            url, depth, delay_range, data, visited, visited_data, ids
        )

    data_features = ['id', 'title', 'url', 'content', 'type']
    return pd.DataFrame(data, columns=data_features)

# ----- PREPROCESSING FUNCTIONS -----

def download_nltk_resources():
    """Download necessary NLTK resources"""
    try:
        nltk.download('punkt')
        nltk.download('stopwords')
    except:
        print("Error downloading NLTK resources. If you're running in Colab, make sure you're connected.")

def preprocess_text(text):
    """
    Preprocess text: lowercase, remove punctuation, numbers, URLs, stem words, remove stopwords

    Args:
        text: Input text string

    Returns:
        Preprocessed text as space-separated string
    """
    if not isinstance(text, str) or pd.isna(text):
        return ""

    # Lowercase
    text = text.lower()

    # Remove punctuation, numbers, URLs
    text = re.sub(r'[^\w\s]', '', text)
    text = re.sub(r'\d+', '', text)
    text = re.sub(r'http\S+', '', text)
    text = re.sub(r'\s+', ' ', text).strip()

    # Tokenize, remove stopwords and stem
    stemmer = PorterStemmer()
    stop_words = set(stopwords.words('english'))
    tokens = word_tokenize(text)
    processed_tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]

    return ' '.join(processed_tokens)  # Return as space-separated string

def remove_common_boilerplate(text, common_sequences):
    """
    Remove common boilerplate text sequences

    Args:
        text: Input text
        common_sequences: List of common text sequences to remove

    Returns:
        Text with boilerplate removed
    """
    if not isinstance(text, str) or pd.isna(text):
        return text

    for sequence in common_sequences:
        text = text.replace(sequence, '')

    return text

def detect_duplicates(texts, threshold=0.9):
    """
    Detect near-duplicate content

    Args:
        texts: List of text strings
        threshold: Similarity threshold for duplicates

    Returns:
        List of indices to keep (non-duplicates)
    """
    # Count frequency of each document
    text_counter = Counter(texts)

    # Find exact duplicates - keep only one copy
    duplicates = set()
    for i, text in enumerate(texts):
        if text_counter[text] > 1 and text != "":
            duplicates.add(i)
            text_counter[text] -= 1

    return [i for i in range(len(texts)) if i not in duplicates]

def preprocess_pipeline(df, common_sequences=None):
    """
    Run complete preprocessing pipeline on a dataframe

    Args:
        df: DataFrame with 'content' column
        common_sequences: List of common boilerplate text sequences to remove

    Returns:
        Preprocessed DataFrame
    """
    # Ensure NLTK resources are available
    download_nltk_resources()

    # Create a copy to avoid modifying the original
    df_processed = df.copy()

    # Default common sequences if none provided
    if common_sequences is None:
        common_sequences = [
            'home news sport busi innov cultur art travel earth audio video live',
            'news sport busi innov cultur art travel earth audio video',
            'home news sport busi innov cultur',
            'travel earth audio video live',
            'rate averag rate star minut log sign save recip easi',
            'code conduct develop statist cooki statement mobil view search',
            'jump content main menu main menu move sidebar hide navig main page',
            'new york time skip content skip site'
        ]

    # Step 1: Remove boilerplate from HTML content
    content_col = 'content'
    mask = df_processed[content_col].notna()
    df_processed.loc[mask, content_col] = df_processed.loc[mask, content_col].apply(
        lambda x: remove_common_boilerplate(x, common_sequences)
    )

    # Step 2: Process main content
    df_processed['content_processed'] = df_processed[content_col].apply(preprocess_text)

    # Step 3: Process titles
    df_processed['title_processed'] = df_processed['title'].apply(preprocess_text)

    # Step 4: Remove duplicates
    non_duplicates = detect_duplicates(df_processed['content_processed'].tolist())
    df_processed = df_processed.iloc[non_duplicates].reset_index(drop=True)

    return df_processed

# ----- INDEXING AND RETRIEVAL FUNCTIONS -----

def create_pyterrier_index(df, index_path='./pt_index'):
    """
    Create a PyTerrier index from the preprocessed data

    Args:
        df: DataFrame with processed content or titles
        index_path: Path to store the index

    Returns:
        PyTerrier index reference
    """
    # Ensure the index directory exists
    os.makedirs(index_path, exist_ok=True)

    # Prepare dataframe for indexing - PyTerrier needs specific column names
    # Check if we have content_processed or need to use title_processed
    if 'content_processed' in df.columns and df['content_processed'].notna().sum() > 0:
        text_col = 'content_processed'
    else:
        text_col = 'title_processed'  # Fall back to title_processed if content not available

    print(f"Using '{text_col}' column for indexing")

    # Select columns and rename
    columns_to_use = ['id', text_col, 'url', 'title']
    available_columns = [col for col in columns_to_use if col in df.columns]

    indexer_df = df[available_columns].copy()

    # Map column names to PyTerrier expected format
    column_mapping = {
        'id': 'docno',
        text_col: 'text',
        'url': 'url',
        'title': 'title'
    }

    # Rename only columns that exist
    for old_col, new_col in column_mapping.items():
        if old_col in indexer_df.columns:
            indexer_df = indexer_df.rename(columns={old_col: new_col})

    # Ensure required columns exist
    if 'docno' not in indexer_df.columns:
        indexer_df['docno'] = range(len(indexer_df))
    if 'text' not in indexer_df.columns:
        print("Error: No text content found for indexing")
        return None

    # Remove rows with empty content
    indexer_df = indexer_df[indexer_df['text'].notna() & (indexer_df['text'] != '')].reset_index(drop=True)

    # Convert docno to string
    indexer_df['docno'] = indexer_df['docno'].astype(str)

    # Add any missing required columns with default values
    for col in ['url', 'title']:
        if col not in indexer_df.columns:
            indexer_df[col] = ['Document ' + str(i) for i in range(len(indexer_df))]

    print(f"Prepared {len(indexer_df)} documents for indexing")

    # Create indexer
    indexer = pt.DFIndexer(index_path, overwrite=True)
    indexer.index(indexer_df['text'], indexer_df)

    print(f"Indexed {len(indexer_df)} documents")

    return pt.IndexFactory.of(index_path)

def build_retrieval_pipeline(index_ref, model_name="BM25", use_query_expansion=False):
    """
    Build a retrieval pipeline using PyTerrier

    Args:
        index_ref: PyTerrier index reference
        model_name: Name of retrieval model ('BM25' or 'TF_IDF')
        use_query_expansion: Whether to use query expansion with RM3

    Returns:
        PyTerrier retrieval pipeline
    """
    # Create base retrieval model
    if model_name == "BM25":
        retrieval = pt.BatchRetrieve(index_ref, wmodel="BM25")
    else:  # Default to TF_IDF
        retrieval = pt.BatchRetrieve(index_ref, wmodel="TF_IDF")

    # Add query expansion if requested
    if use_query_expansion:
        pipeline = retrieval >> pt.rewrite.RM3(index_ref) >> retrieval
    else:
        pipeline = retrieval

    return pipeline

def run_search(query, df, pipeline, stemmer=None, stop_words=None, num_results=10):
    """
    Run a search query through the retrieval pipeline

    Args:
        query: User query string
        df: Original DataFrame with document data
        pipeline: PyTerrier retrieval pipeline
        stemmer: Stemmer to use for query processing
        stop_words: Set of stopwords to remove
        num_results: Number of results to return

    Returns:
        DataFrame with search results
    """
    # Preprocess query similar to documents
    if stemmer and stop_words:
        query = query.lower()
        query = re.sub(r'[^\w\s]', '', query)
        tokens = word_tokenize(query)
        processed_tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]
        processed_query = ' '.join(processed_tokens)
    else:
        processed_query = query

    # Run query through pipeline
    results = pipeline.search(processed_query)

    if len(results) == 0:
        return pd.DataFrame(columns=['rank', 'score', 'title', 'url', 'snippet'])

    # Limit to top N results
    results = results.head(num_results)

    # Map docno back to original data
    id_to_doc = {str(row['id']): row for _, row in df.iterrows()}

    # Prepare results dataframe
    search_results = []
    for i, (_, row) in enumerate(results.iterrows()):
        docno = row['docno']
        if docno in id_to_doc:
            doc = id_to_doc[docno]

            # Check if it's an image (type column exists and equals 'image')
            is_image_item = 'type' in doc and doc['type'] == 'image'

            # Generate snippet based on content or title
            if 'content' in doc and doc['content'] and not pd.isna(doc['content']):
                # Text content available
                content = str(doc['content'])
                snippet = content[:200] + "..." if len(content) > 200 else content
            elif is_image_item:
                # It's an image with no content
                snippet = "[IMAGE] " + str(doc.get('title', 'No title'))
            else:
                # Fallback to title
                snippet = str(doc.get('title', 'No title'))

            search_results.append({
                'rank': i+1,
                'score': round(float(row['score']), 4),
                'title': doc.get('title', 'No title'),
                'url': doc.get('url', 'No URL'),
                'snippet': snippet,
                'type': doc.get('type', 'text')
            })

    return pd.DataFrame(search_results)

# ----- GRADIO GUI FUNCTIONS -----

def create_gui(df, index_ref):
    """
    Create and launch Gradio GUI for the search system

    Args:
        df: DataFrame with document data
        index_ref: PyTerrier index reference

    Returns:
        Gradio interface
    """
    # Prepare stemmer and stopwords for query processing
    stemmer = PorterStemmer()
    stop_words = set(stopwords.words('english'))

    # Create retrieval pipelines
    bm25_pipeline = build_retrieval_pipeline(index_ref, "BM25", False)
    bm25_rm3_pipeline = build_retrieval_pipeline(index_ref, "BM25", True)
    tfidf_pipeline = build_retrieval_pipeline(index_ref, "TF_IDF", False)
    tfidf_rm3_pipeline = build_retrieval_pipeline(index_ref, "TF_IDF", True)

    def search_function(query, model_selection):
        """Inner function to process search in the GUI"""
        if not query.strip():
            return pd.DataFrame(columns=['rank', 'score', 'title', 'url', 'snippet', 'type'])

        # Select the appropriate pipeline based on user choice
        if model_selection == "BM25":
            pipeline = bm25_pipeline
        elif model_selection == "BM25 + Query Expansion (RM3)":
            pipeline = bm25_rm3_pipeline
        elif model_selection == "TF-IDF":
            pipeline = tfidf_pipeline
        else:  # TF-IDF + RM3
            pipeline = tfidf_rm3_pipeline

        return run_search(query, df, pipeline, stemmer, stop_words)

    def display_results(search_results):
        """Display formatted search results with image previews"""
        if search_results.empty:
            return "No results found."

        result_html = ""

        for _, row in search_results.iterrows():
            result_type = row.get('type', 'text')
            title = row['title']
            url = row['url']
            score = row['score']

            result_html += f"<div style='margin-bottom: 20px; border-bottom: 1px solid #eee; padding-bottom: 15px;'>"
            result_html += f"<h3>{title}</h3>"

            # Display image preview for image types
            if result_type == 'image':
                result_html += f"<div style='margin: 10px 0;'>"
                result_html += f"<img src='{url}' style='max-height: 200px; max-width: 100%;' onerror=\"this.onerror=null; this.src='https://via.placeholder.com/300x200?text=Image+Not+Available';\"/>"
                result_html += "</div>"

            # Display snippet
            if 'snippet' in row and row['snippet']:
                snippet = row['snippet']
                if result_type != 'image':  # Don't show snippet for images since we show the image
                    result_html += f"<p>{snippet}</p>"

            # Show metadata
            result_html += f"<div style='color: #888; font-size: 0.9em;'>"
            result_html += f"Score: {score} | <a href='{url}' target='_blank'>View source</a>"
            result_html += "</div>"

            result_html += "</div>"

        return result_html

    # Create the Gradio interface
    with gr.Blocks(title="Smart IR System") as demo:
        gr.Markdown("# Smart Information Retrieval System")
        gr.Markdown("### Search over the crawled collection using different retrieval models")

        with gr.Row():
            with gr.Column(scale=4):
                query_input = gr.Textbox(
                    label="Enter your search query",
                    placeholder="Type your search query here..."
                )
            with gr.Column(scale=1):
                model_dropdown = gr.Dropdown(
                    ["BM25", "BM25 + Query Expansion (RM3)", "TF-IDF", "TF-IDF + Query Expansion (RM3)"],
                    label="Retrieval Model",
                    value="BM25"
                )
                search_button = gr.Button("Search")

        # Two outputs - raw results dataframe and HTML formatted results
        with gr.Row():
            with gr.Column():
                results_html = gr.HTML(label="Search Results")

        with gr.Accordion("Raw Results Data", open=False):
            results_df = gr.DataFrame(
                headers=["Rank", "Score", "Title", "URL", "Snippet", "Type"],
                datatype=["number", "number", "str", "str", "str", "str"],
                label="Raw Results Data"
            )

        # Connect components - pipeline search → results → display
        search_click = search_button.click(
            fn=search_function,
            inputs=[query_input, model_dropdown],
            outputs=[results_df]
        )

        search_click.then(
            fn=display_results,
            inputs=[results_df],
            outputs=[results_html]
        )

        # Also handle pressing Enter in the query box
        query_submit = query_input.submit(
            fn=search_function,
            inputs=[query_input, model_dropdown],
            outputs=[results_df]
        )

        query_submit.then(
            fn=display_results,
            inputs=[results_df],
            outputs=[results_html]
        )

    return demo

# ----- TESTING FUNCTIONS -----

def test_preprocessor():
    """Test the preprocessing pipeline"""
    print("Testing preprocessor...")
    test_text = "This is a Test document about Information Retrieval. It contains some URLs like https://example.com and numbers 123."
    processed = preprocess_text(test_text)

    assert len(processed) < len(test_text), "Text wasn't reduced in preprocessing"
    assert "test" in processed, "Lowercasing failed"
    assert "https" not in processed, "URL removal failed"
    assert "123" not in processed, "Number removal failed"
    print("Preprocessor test passed.")
    return True

def test_ir_model(index_ref):
    """Test the IR model pipeline"""
    print("Testing IR models...")
    # Find a common term in the indexed data to test with
    common_terms = ["news", "information", "photo", "image", "pope", "putin", "xi"]

    # Test BM25
    bm25 = build_retrieval_pipeline(index_ref, "BM25", False)

    # Try different terms until we get results
    bm25_results = None
    test_term = None
    for term in common_terms:
        results = bm25.search(term)
        if len(results) > 0:
            bm25_results = results
            test_term = term
            break

    if bm25_results is None:
        print("Warning: Could not find any search terms that return results. Using 'a' as fallback.")
        test_term = "a"  # Fallback to a very common term
        bm25_results = bm25.search(test_term)

    # Test TF-IDF with the same term
    tfidf = build_retrieval_pipeline(index_ref, "TF_IDF", False)
    results_tfidf = tfidf.search(test_term)

    # Test Query Expansion
    bm25_rm3 = build_retrieval_pipeline(index_ref, "BM25", True)
    results_rm3 = bm25_rm3.search(test_term)

    print(f"Testing with term: '{test_term}'")
    print(f"BM25 results: {len(bm25_results)}")
    print(f"TF-IDF results: {len(results_tfidf)}")
    print(f"BM25+RM3 results: {len(results_rm3)}")

    assert len(bm25_results) > 0, "BM25 returned no results"
    assert len(results_tfidf) > 0, "TF-IDF returned no results"
    # RM3 might not always return results if the index is small
    # So we'll just warn instead of failing the test
    if len(results_rm3) == 0:
        print("Warning: BM25+RM3 returned no results. This may be due to limited data.")

    print("IR models test passed.")
    return True

def run_tests(index_ref=None):
    """Run all tests"""
    test_preprocessor()
    if index_ref:
        test_ir_model(index_ref)
    print("All tests passed!")

# ----- VECTOR SPACE MODEL IMPLEMENTATION -----
# This is included as a reference implementation of the VSM concepts
# The main search functionality uses PyTerrier's implementations

def compute_tf_idf_vectors(documents):
    """
    Compute TF-IDF vectors for documents (manual implementation)

    Args:
        documents: List of preprocessed document texts

    Returns:
        Document vectors, vocabulary
    """
    # Build vocabulary
    vocabulary = set()
    for doc in documents:
        tokens = doc.split()
        vocabulary.update(tokens)

    vocabulary = sorted(list(vocabulary))
    vocab_index = {term: i for i, term in enumerate(vocabulary)}

    # Compute term frequencies (TF)
    tf_vectors = np.zeros((len(documents), len(vocabulary)))
    for i, doc in enumerate(documents):
        tokens = doc.split()
        for token in tokens:
            j = vocab_index[token]
            tf_vectors[i, j] += 1

    # Normalize TF by document length
    doc_lengths = np.sum(tf_vectors, axis=1)
    doc_lengths[doc_lengths == 0] = 1  # Avoid division by zero
    tf_vectors = tf_vectors / doc_lengths[:, np.newaxis]

    # Compute document frequencies (DF)
    df = np.sum(tf_vectors > 0, axis=0)

    # Compute IDF
    N = len(documents)
    idf = np.log(N / (df + 1))  # +1 for smoothing

    # Compute TF-IDF
    tfidf_vectors = tf_vectors * idf

    return tfidf_vectors, vocabulary, vocab_index

def compute_query_vector(query, vocab_index, idf):
    """
    Compute TF-IDF vector for a query (manual implementation)

    Args:
        query: Preprocessed query string
        vocab_index: Vocabulary index mapping
        idf: IDF values

    Returns:
        Query vector
    """
    query_vector = np.zeros(len(vocab_index))
    tokens = query.split()

    # Compute TF
    for token in tokens:
        if token in vocab_index:
            j = vocab_index[token]
            query_vector[j] += 1

    # Normalize TF
    length = np.sum(query_vector)
    if length > 0:
        query_vector = query_vector / length

    # Apply IDF weights
    query_vector = query_vector * idf

    return query_vector

def cosine_similarity(v1, v2):
    """
    Compute cosine similarity between two vectors

    Args:
        v1, v2: Input vectors

    Returns:
        Cosine similarity score
    """
    dot_product = np.dot(v1, v2)
    norm_v1 = np.linalg.norm(v1)
    norm_v2 = np.linalg.norm(v2)

    if norm_v1 == 0 or norm_v2 == 0:
        return 0

    return dot_product / (norm_v1 * norm_v2)

def manual_vsm_search(query, documents, doc_metadata):
    """
    Perform search using manually implemented Vector Space Model

    This is included only as a reference implementation of VSM concepts.
    The actual search uses PyTerrier's optimized implementations.

    Args:
        query: Preprocessed query string
        documents: List of preprocessed document texts
        doc_metadata: List of document metadata dictionaries

    Returns:
        List of search results
    """
    # Compute document vectors
    doc_vectors, vocabulary, vocab_index = compute_tf_idf_vectors(documents)

    # Compute IDF values (reuse from doc_vectors calculation)
    N = len(documents)
    df = np.sum(doc_vectors > 0, axis=0)
    idf = np.log(N / (df + 1))

    # Compute query vector
    query_vector = compute_query_vector(query, vocab_index, idf)

    # Compute similarities
    similarities = []
    for i, doc_vector in enumerate(doc_vectors):
        sim = cosine_similarity(query_vector, doc_vector)
        similarities.append((i, sim))

    # Sort by similarity score (descending)
    similarities.sort(key=lambda x: x[1], reverse=True)

    # Format results
    results = []
    for i, sim in similarities[:10]:  # Top 10 results
        if sim > 0:
            results.append({
                'rank': len(results) + 1,
                'score': round(float(sim), 4),
                'title': doc_metadata[i].get('title', 'No title'),
                'url': doc_metadata[i].get('url', 'No URL'),
                'snippet': doc_metadata[i].get('content', '')[:200] + "..."
            })

    return results

# ----- MAIN EXECUTION FLOW -----

def main():
    """Main execution function"""
    print("Starting Smart Information Retrieval System...")

    # 1. Load existing data from final.csv
    print("Loading data from final.csv...")
    try:
        df = pd.read_csv('final.csv')
        print(f"Loaded {len(df)} items from final.csv")
    except FileNotFoundError:
        print("Error: final.csv not found. Please ensure the file exists in the current directory.")
        return
    except Exception as e:
        print(f"Error loading data: {str(e)}")
        return

    # Print data info
    print("\nData columns:", df.columns.tolist())
    print(f"Data shape: {df.shape}")
    print("\nSample data (first 5 rows):")
    print(df.head())

    # Since the data is already preprocessed (has title_processed column),
    # we'll use it directly for indexing
    print("\nPreparing data for indexing...")

    # 2. Indexing
    print("Creating search index...")
    index_ref = create_pyterrier_index(df)
    print("Indexing complete.")

    # 3. Run tests (skip crawler test since we're not crawling)
    print("Running tests...")
    test_preprocessor()
    test_ir_model(index_ref)
    print("Tests complete.")

    # 4. Launch GUI
    print("Launching search interface...")
    demo = create_gui(df, index_ref)
    demo.launch(share=True)

if __name__ == "__main__":
    # Start the system
    main()

"""#2"""

import gradio as gr
import pandas as pd

def search_function(query, model_selection, alpha_value=0.7, enable_clustering=False, enable_diversity=False):
    """Placeholder function to process search in the GUI"""
    # This is just a placeholder - integrate with your actual search functionality
    if not query.strip():
        return pd.DataFrame(columns=['rank', 'score', 'title', 'url', 'snippet', 'type'])

    # Return sample data for demonstration
    sample_data = {
        'rank': [1, 2, 3],
        'score': [0.95, 0.85, 0.75],
        'title': ['Sample Result 1', 'Sample Result 2', 'Sample Result 3'],
        'url': ['http://example.com/1', 'http://example.com/2', 'http://example.com/3'],
        'snippet': ['This is a sample snippet...', 'Another sample snippet...', 'Yet another sample...'],
        'type': ['text', 'image', 'text']
    }

    return pd.DataFrame(sample_data)

def display_results(search_results):
    """Display formatted search results with image previews and optional clustering"""
    if search_results.empty:
        return "No results found."

    result_html = ""

    # Check if we have cluster information
    has_clusters = 'cluster' in search_results.columns

    if has_clusters:
        # Group by cluster
        clusters = search_results['cluster'].unique()
        clusters = sorted(clusters)

        for cluster_id in clusters:
            cluster_results = search_results[search_results['cluster'] == cluster_id]

            # Generate a label for the cluster (could be enhanced with topic modeling)
            cluster_label = f"Cluster {cluster_id+1}"

            result_html += f"<div style='margin: 15px 0; padding: 10px; background-color: #f8f9fa; border-radius: 5px;'>"
            result_html += f"<h2>{cluster_label}</h2>"

            # Show results in this cluster
            for _, row in cluster_results.iterrows():
                result_html += format_result_item(row)

            result_html += "</div>"
    else:
        # Regular non-clustered display
        for _, row in search_results.iterrows():
            result_html += format_result_item(row)

    return result_html

def format_result_item(row):
    """Format a single search result item"""
    result_type = row.get('type', 'text')
    title = row['title']
    url = row['url']
    score = row['score']

    item_html = f"<div style='margin-bottom: 20px; border-bottom: 1px solid #eee; padding-bottom: 15px;'>"
    item_html += f"<h3>{title}</h3>"

    # Display image preview for image types
    if result_type == 'image':
        item_html += f"<div style='margin: 10px 0;'>"
        item_html += f"<img src='{url}' style='max-height: 200px; max-width: 100%;' onerror=\"this.onerror=null; this.src='https://via.placeholder.com/300x200?text=Image+Not+Available';\"/>"
        item_html += "</div>"

    # Display snippet
    if 'snippet' in row and row['snippet']:
        snippet = row['snippet']
        if result_type != 'image':  # Don't show snippet for images since we show the image
            item_html += f"<p>{snippet}</p>"

    # Show metadata
    item_html += f"<div style='color: #888; font-size: 0.9em;'>"
    item_html += f"Score: {score:.4f} | <a href='{url}' target='_blank'>View source</a>"
    item_html += "</div>"

    item_html += "</div>"

    return item_html

# Create the Gradio interface
with gr.Blocks(title="Smart IR System with Embeddings") as demo:
    gr.Markdown("# Smart Information Retrieval System")
    gr.Markdown("### Search with traditional retrieval models and neural embeddings")

    with gr.Row():
        with gr.Column(scale=4):
            query_input = gr.Textbox(
                label="Enter your search query",
                placeholder="Type your search query here..."
            )
        with gr.Column(scale=2):
            model_options = ["BM25", "BM25 + Query Expansion (RM3)", "TF-IDF",
                             "Embedding-based", "Hybrid (BM25 + Embeddings)"]

            model_dropdown = gr.Dropdown(
                model_options,
                label="Retrieval Model",
                value="BM25"
            )

    with gr.Row():
        with gr.Column():
            search_button = gr.Button("Search", variant="primary")
        with gr.Column():
            alpha_slider = gr.Slider(
                minimum=0.0,
                maximum=1.0,
                value=0.7,
                step=0.1,
                label="BM25 vs Embedding Weight (Hybrid only)",
                info="Higher values give more weight to BM25 results"
            )

    with gr.Row():
        with gr.Column():
            enable_clustering = gr.Checkbox(
                label="Enable Result Clustering",
                info="Group similar results together"
            )
        with gr.Column():
            enable_diversity = gr.Checkbox(
                label="Enable Diversity Reranking",
                info="Rerank results for better topic coverage"
            )

    # Results display
    with gr.Row():
        with gr.Column():
            results_html = gr.HTML(label="Search Results")

    with gr.Accordion("Raw Results Data", open=False):
        results_df = gr.DataFrame(
            headers=["Rank", "Score", "Title", "URL", "Snippet", "Type"],
            label="Raw Results Data"
        )

    # Connect components - full search flow
    search_click = search_button.click(
        fn=search_function,
        inputs=[query_input, model_dropdown, alpha_slider, enable_clustering, enable_diversity],
        outputs=[results_df]
    )

    search_click.then(
        fn=display_results,
        inputs=[results_df],
        outputs=[results_html]
    )

demo.launch()

