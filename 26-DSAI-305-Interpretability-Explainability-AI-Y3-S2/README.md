# DSAI 305 - Interpretability & Explainability in AI (Y3-S2)

<div align="center">

**Spring 2024** | **Advanced AI Course** | **Research Publication**

[![Materials](https://img.shields.io/badge/Materials-Complete-success)](.)
[![Projects](https://img.shields.io/badge/Projects-Research-blue)](.)
[![Research](https://img.shields.io/badge/Research-Published-success)](.)
[![Skills](https://img.shields.io/badge/Skills-XAI%20|%20SHAP%20|%20LIME-orange)](.)

</div>

---

## ğŸ“‹ Course Overview

Advanced course on interpretable and explainable AI covering model interpretation techniques, post-hoc explanations, and inherently interpretable models. Emphasis on building trustworthy AI systems with transparent decision-making processes.

**Research Output**: Published paper on financial sentiment analysis with explainability

---

## ğŸ¯ Learning Outcomes Achieved

### Interpretability Techniques
- âœ… Feature importance methods
- âœ… SHAP (SHapley Additive exPlanations)
- âœ… LIME (Local Interpretable Model-agnostic Explanations)
- âœ… Attention visualization

### Explainable Models
- âœ… Inherently interpretable models
- âœ… Rule-based systems
- âœ… Decision trees and GAMs
- âœ… Concept-based explanations

### Evaluation & Trust
- âœ… Explanation evaluation metrics
- âœ… Human-in-the-loop evaluation
- âœ… Trustworthiness assessment
- âœ… Regulatory requirements (GDPR, AI Act)

---

## ğŸ“š Topics Covered

### Module 1: Introduction to XAI (Weeks 1-3)
- Why explainability matters
- Taxonomy of XAI methods
- Trade-offs: accuracy vs interpretability
- Regulatory landscape

### Module 2: Post-hoc Explanations (Weeks 4-7)
- Feature attribution methods
- SHAP theory and implementation
- LIME and local explanations
- Counterfactual explanations

### Module 3: Interpretable Models (Weeks 8-10)
- Linear models and regularization
- Decision trees and rule lists
- Generalized additive models
- Attention mechanisms

### Module 4: Evaluation & Applications (Weeks 11-14)
- Evaluation frameworks
- Domain-specific XAI (healthcare, finance)
- Human-AI interaction
- Future directions

---

## ğŸ’» Research Projects

### Research: FinSentImpact - News-Driven Multi-Stock Forecasting

**Published Research Paper**

**Objective**: Develop explainable framework for financial sentiment analysis and stock prediction

**Implementation**:
- Multi-stock forecasting with news sentiment
- SHAP-based explanation generation
- Interpretable attention mechanisms
- Comprehensive evaluation framework

**Key Contributions**:
- âœ… Novel explainable financial NLP framework
- âœ… Multi-modal integration (news + market data)
- âœ… Interpretable predictions for trading decisions
- âœ… Comprehensive ablation studies

[ğŸ“„ Research Paper](./Research%20Paper/) | [ğŸ“‚ GitHub Repository](https://github.com/Abdulrahmann-Omar)

**Technologies**: PyTorch, Transformers, SHAP, LIME, Pandas

---

## ğŸ› ï¸ Technical Skills Developed

### XAI Libraries
- **SHAP**: Shapley value explanations
- **LIME**: Local explanations
- **Captum**: PyTorch interpretability
- **Alibi**: Additional explanation methods

### Visualization
- **Matplotlib/Seaborn**: Explanation plots
- **Plotly**: Interactive dashboards
- **Attention Visualization**: Transformer analysis

---

## ğŸ“– Key Resources

### Textbooks & Papers
- **"Interpretable Machine Learning"** - Christoph Molnar
- **"Explainable AI"** - Longo et al.
- Key papers: SHAP (Lundberg), LIME (Ribeiro)

---

## ğŸ¯ Course Impact on Graduate Studies Preparation

This course directly led to research publication:
- Strong foundation in XAI methods
- Research paper writing and publication
- Critical evaluation of explanation quality
- Understanding of regulatory requirements

---

<div align="center">

**Key Takeaway**: Explainability is not optional - it's essential for trustworthy AI.

[â¬…ï¸ Back to All Courses](../README.md#-complete-course-catalog)

</div>
